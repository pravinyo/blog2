[ { "title": "Understanding Apache Camel: A Comprehensive Guide to Integration Patterns and System Management", "url": "/posts/apache-camel-application-integration/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment", "date": "2025-02-08 00:00:00 +0530", "snippet": "Table of Contents Introduction to Apache Camel Understanding Camel Routes Essential Integration Patterns Real-World Integration Examples Advanced System Management Health CheckIntroductionApache Camel is like a universal translator and message router for different software systems. Think of it as a postal service for your applications - it picks up messages from one place, maybe transforms them, and delivers them to another place.flowchart LR subgraph \"Sources\" Files[Files in Folder] REST[Web Service] Email[Email] SQueue[Message Queue] end subgraph \"Apache Camel\" Router[Message Router] end subgraph \"Destinations\" DB[(Database)] Queue[Message Queue] API[Another Web Service] end Files --&gt; Router REST --&gt; Router Email --&gt; Router SQueue --&gt; Router Router --&gt; DB Router --&gt; Queue Router --&gt; API style Router fill:#f96,stroke:#333Understanding Camel RoutesA Camel route is like a set of instructions telling Camel how to move messages from point A to point B. Here’s a simple example:// This route moves files from an 'orders' folder to a 'processed' folderfrom(\"file:orders\") // 1. Watch the 'orders' folder for new files .log(\"Found file: ${file:name}\") // 2. Log the filename .to(\"file:processed\"); // 3. Move the file to 'processed' folder// This route reads files and sends important ones to a different placefrom(\"file:inbox\") // 1. Watch the 'inbox' folder .choice() // 2. Make a decision based on file content .when(simple(\"${file:name} contains 'urgent'\")) // 3. If filename has 'urgent' .to(\"direct:urgent\") // 4. Send to urgent processor .otherwise() // 5. If not urgent .to(\"direct:normal\"); // 6. Send to normal processorLet’s break down what this code does: The first route is like having someone watch a folder called “orders” for new files, and when they find one, they log its name and move it to a “processed” folder. The second route shows how to make decisions - it’s like a mail sorter who looks at each envelope and sends urgent mail to one place and regular mail to another.Essential Integration Patterns1. Content-Based RouterRoutes messages based on their content:from(\"file:incoming-orders\") .choice() .when(simple(\"${body.orderType} == 'RUSH'\")) .to(\"direct:rush-orders\") .otherwise() .to(\"direct:regular-orders\") .end();This pattern is like a traffic controller that: Examines the content of each message (the orderType) Routes rush orders one way Routes regular orders another way2. Filter PatternFilters messages based on criteria:flowchart LR Input[Input Messages] --&gt; Filter{Contains 'important'?} Filter --&gt;|Yes| Output[Important Queue] Filter --&gt;|No| Discard[Discarded] style Filter fill:#f96,stroke:#333from(\"direct:start\") .filter(simple(\"${body} contains 'important'\")) .to(\"direct:important-messages\");This pattern works like a sieve that: Examines each message Only lets through messages matching certain criteria Discards or ignores non-matching messages3. Transformer PatternConverts messages between formats:flowchart LR Input[JSON Input] --&gt; Transform[Transform Process] Transform --&gt; Output[XML Output] subgraph \"Transform Process\" M[Marshal] --&gt; U[Unmarshal] end style Transform fill:#69f,stroke:#333from(\"direct:start\") .marshal().json() .unmarshal().xml() .to(\"direct:xml-processor\");This pattern acts like a format converter that: Takes input in one format (like JSON) Transforms it to another format (like XML) Useful when connecting systems that speak different “languages”4. Dead Letter Channel PatternHandles failed messages by moving them to a Dead Letter Queue (DLQ).flowchart TD Start[Message] --&gt; Process{Process Message} Process --&gt;|Success| Success[Success Queue] Process --&gt;|Failure| Retry{Retry?} Retry --&gt;|Yes| Process Retry --&gt;|No| Dead[Dead Letter Queue] style Process fill:#f69,stroke:#333 style Retry fill:#f69,stroke:#333errorHandler(deadLetterChannel(\"direct:dead-letter\") .maximumRedeliveries(3) .redeliveryDelay(1000) .backOffMultiplier(2));from(\"direct:start\") .to(\"direct:possibly-failing-service\") .log(\"Message processed successfully\");This pattern works like a safety net that: Attempts to process a message. If processing fails, retries up to 3 times. Increases the delay between retries. If all retries fail, moves the message to a “dead letter” queue for further investigation.5. Splitter PatternSplits a single message into multiple messages:from(\"direct:start\") .split(body().tokenize(\",\")) .to(\"direct:process-split-message\");This pattern works like a paper cutter that: Takes a single message containing multiple items (e.g., a comma-separated list) Splits it into individual messages for each item Processes each item separately6. Aggregator PatternCombines multiple messages into a single message:from(\"direct:start\") .aggregate(header(\"orderId\"), new GroupedExchangeAggregationStrategy()) .completionSize(5) .to(\"direct:process-aggregated-message\");This pattern works like a collector that: Groups messages based on a common attribute (e.g., orderId) Aggregates them into a single message once a condition is met (e.g., 5 messages received) Processes the aggregated message7. Wire Tap Pattern (Logging Implementation)from(\"file:incoming-orders\") .wireTap(\"direct:audit-log\") .log(\"Received order: ${file:name}\") .to(\"direct:process-order\");from(\"direct:audit-log\") .log(\"Audit: ${file:name} received at ${date:now:yyyy-MM-dd HH:mm:ss}\") .to(\"file:audit-logs\");This pattern works like a phone tap that: Creates a copy of each message Sends the copy to a logging/monitoring system Allows the original message to continue its journey Useful for audit trails and monitoring8. Error handlerfrom(\"direct:start\") .errorHandler(defaultErrorHandler() .maximumRedeliveries(3) .redeliveryDelay(1000)) .to(\"direct:possibly-failing-service\") .log(\"Message processed successfully\");This pattern help implement simple error handling, Like a persistent delivery person who tries multiple times if nobody answers the door If sending a message fails, it will retry up to 3 times Between each retry, it waits for 1 second If all retries fail, it gives up and reports an errorReal-World Integration ExamplesHere’s an example that tries to show different patterns in use.// Order Processing Routefrom(\"file:incoming-orders\") // 1. Watch 'incoming-orders' folder for new files .log(\"Received order: ${file:name}\") // 2. Log when we find a new order // 3. Convert the file content to JSON .unmarshal().json() // 4. Add current timestamp to the order .setHeader(\"ProcessingTime\", simple(\"${date:now:yyyy-MM-dd HH:mm:ss}\")) // 5. Make decisions based on order type .choice() .when(simple(\"${body.orderType} == 'RUSH'\")) .log(\"Rush order detected!\") .to(\"direct:rush-orders\") .otherwise() .log(\"Regular order received\") .to(\"direct:regular-orders\") .end() // 6. Move processed file to archive .to(\"file:processed-orders\");// Rush Order Handlerfrom(\"direct:rush-orders\") .log(\"Processing rush order\") .to(\"direct:notify-shipping\") .to(\"direct:update-inventory\");// Regular Order Handlerfrom(\"direct:regular-orders\") .log(\"Processing regular order\") .delay(simple(\"${random(1000,5000)}\")) // Simulate processing time .to(\"direct:update-inventory\");Let’s break down what this order processing system does: File Monitoring: Watches a folder called “incoming-orders” for new order files When a new file appears, it starts processing it Initial Processing: Logs the filename so we know which order is being processed Converts the file content from JSON format (like a digital order form) Adding Information: Adds a timestamp to track when the order was processed Decision Making: Checks if it’s a rush order or regular order Rush orders get special handling Regular orders follow the normal process Different Handling: Rush orders: Immediately notify shipping and update inventory Regular orders: Process with some delay and update inventory File Management: Moves the processed order file to a different folder for record-keeping This example shows how Camel can: Monitor folders for new files Make decisions based on content Process different types of orders differently Keep track of what’s happening (logging) Archive processed filesIn previous example, different patterns work together to create a robust system: Content-Based Router sorts orders into rush and regular Wire Tap logs all activities for monitoring Transformer converts file contents into processable formats Dead Letter Channel handles any processing failures Filter could be used to only process orders meeting certain criteriaThese patterns are like building blocks that can be combined in different ways to handle various integration scenarios. They help solve common problems in a standardized way, making the system more maintainable and reliable.Kafka to ActiveMQ IntegrationThis example demonstrates consuming messages from Kafka and pushing them to ActiveMQ:import org.apache.camel.builder.RouteBuilder;import org.apache.camel.component.kafka.KafkaConstants;import org.apache.camel.LoggingLevel;public class KafkaToMQRoute extends RouteBuilder { @Override public void configure() throws Exception { // Error Handler Configuration errorHandler(deadLetterChannel(\"activemq:queue:dead.letter\") .maximumRedeliveries(3) .redeliveryDelay(1000) .backOffMultiplier(2) .useExponentialBackOff() .logRetryAttempted(true)); // Main route: Kafka → ActiveMQ from(\"kafka:orders-topic?\" + \"brokers=localhost:9092\" + \"&amp;groupId=orders-group\" + \"&amp;autoOffsetReset=earliest\" + \"&amp;consumersCount=3\" + \"&amp;maxPollRecords=100\") // Add correlation ID for tracking .setHeader(\"correlationId\", simple(\"${random(1000,9999)}\")) .log(LoggingLevel.INFO, \"Received from Kafka: ${body} with key: ${headers[kafka.KEY]}\") .process(exchange -&gt; { // Example message transformation String kafkaMessage = exchange.getIn().getBody(String.class); OrderMessage order = convertToOrderMessage(kafkaMessage); exchange.getIn().setBody(order); }) .setHeader(\"JMSPriority\", simple(\"${headers[kafka.PRIORITY]}\")) // Send to ActiveMQ queue .to(\"activemq:queue:orders?\" + \"timeToLive=86400000\" + \"&amp;deliveryPersistent=true\") .log(LoggingLevel.INFO, \"Successfully sent to ActiveMQ: ${header.correlationId}\"); // Dead Letter Queue Handler from(\"activemq:queue:dead.letter\") .log(LoggingLevel.ERROR, \"Failed to process message: ${body}. Error: ${exception.message}\") .process(handleError) .to(\"activemq:queue:failed.orders\"); } private OrderMessage convertToOrderMessage(String kafkaMessage) { //conversion logic here return new OrderMessage(/* converted message */); }}@Getter@Setter@Builderclass OrderMessage { private String orderId; private String customerInfo; private Double amount;}Let’s break down this example in detail: Main Components: Kafka Consumer: Reads messages from a Kafka topic named “orders-topic” Message Transformer: Converts Kafka messages to ActiveMQ format ActiveMQ Producer: Sends messages to ActiveMQ queue named “orders” Kafka Consumer Configuration: \"kafka:orders-topic?brokers=localhost:9092\" + \"&amp;groupId=orders-group\" + \"&amp;autoOffsetReset=earliest\" + \"&amp;consumersCount=3\" + \"&amp;maxPollRecords=100\" brokers: Kafka server address groupId: Consumer group for load balancing autoOffsetReset: Start reading position consumersCount: Number of parallel consumers maxPollRecords: Batch size for each poll Error Handling: errorHandler(deadLetterChannel(\"activemq:queue:dead.letter\") Retries failed messages 3 times Uses exponential backoff (delays increase between retries) Failed messages go to a dead letter queue Message Processing: Adds correlation ID for tracking Logs incoming messages Transforms message format Sets JMS properties ActiveMQ Configuration: activemq:queue:orders?timeToLive=86400000 Sets message expiry time (24 hours) Uses persistent delivery for reliability Here’s a visualization of the message flow:flowchart LR subgraph Kafka KT[Orders Topic] end subgraph \"Camel Route\" C[Consumer] T[Transform] P[Producer] end subgraph ActiveMQ Q[Orders Queue] DL[Dead Letter Queue] end KT --&gt; C C --&gt; T T --&gt; P P --&gt; Q P -.-&gt; DL style C fill:#f96,stroke:#333 style T fill:#69f,stroke:#333 style P fill:#9f6,stroke:#333To use this route, you would need: Dependencies in your pom.xml:&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel&lt;/groupId&gt; &lt;artifactId&gt;camel-kafka&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel&lt;/groupId&gt; &lt;artifactId&gt;camel-activemq&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Configure connection properties in your application.properties:kafka.bootstrap.servers=localhost:9092activemq.broker.url=tcp://localhost:61616Some key features of this implementation: Reliable message delivery with retry mechanism Message tracking with correlation IDs Comprehensive logging Dead letter queue for failed messages Scalable with multiple consumers Message transformation capabilityAdvanced System ManagementSystem outages are common and can impact your services. One common approach is to use circuit breakers and route-level control to manage system outages.Circuit Breaker// Order Processing Route with Circuit Breakerfrom(\"kafka:orders-topic\") .routeId(\"order-processing-route\") .circuitBreaker() .resilience4jConfiguration() .failureRateThreshold(50) // 50% failure rate to open circuit .waitDurationInOpenState(10000) // Wait 10 seconds in open state .end() .to(\"http://order-service/api/orders\") .onFallback() .to(\"kafka:failed-orders\") .end();This code sets up a route in Apache Camel that reads messages from a Kafka topic named orders-topic. It uses a circuit breaker pattern to handle failures gracefully. If the failure rate exceeds 50%, the circuit breaker will open and stop sending requests to the primary endpoint (http://order-service/api/orders) for 10 seconds. During this time, any incoming messages will be sent to a fallback Kafka topic named failed-orders. This helps to prevent overwhelming the order service with requests when it is experiencing issues.Implementing the circuit breaker pattern in a route is quite straightforward as it is provided out of the box by Camel.Let’s take another example to understand how we can apply this in a Camel route.import org.apache.camel.builder.RouteBuilder;import org.apache.camel.model.CircuitBreakerDefinition;import org.apache.camel.builder.DeadLetterChannelBuilder;import org.apache.camel.LoggingLevel;public class MultiRouteWithCircuitBreaker extends RouteBuilder { private static final int THRESHOLD = 5; private static final int HALF_OPEN_AFTER = 10000; @Override public void configure() throws Exception { // Global error handler errorHandler(deadLetterChannel(\"direct:errorHandler\") .maximumRedeliveries(3) .redeliveryDelay(1000) .useExponentialBackOff() .logRetryAttempted(true)); // Route Control and Monitoring from(\"timer:routeMonitor?period=60000\") .routeId(\"monitor-route\") .process(exchange -&gt; { checkDestinationHealth(exchange); }) .choice() .when(header(\"systemStatus\").isEqualTo(\"DOWN\")) .process(exchange -&gt; { // Stop affected routes getContext().getRouteController().stopRoute(\"order-processing-route\"); getContext().getRouteController().stopRoute(\"payment-processing-route\"); }) .otherwise() .process(exchange -&gt; { // Start routes if they're stopped if (!getContext().getRouteController().getRouteStatus(\"order-processing-route\").isStarted()) { getContext().getRouteController().startRoute(\"order-processing-route\"); } if (!getContext().getRouteController().getRouteStatus(\"payment-processing-route\").isStarted()) { getContext().getRouteController().startRoute(\"payment-processing-route\"); } }); // Order Processing Route with Circuit Breaker from(\"kafka:orders-topic\") .routeId(\"order-processing-route\") .circuitBreaker() .resilience4jConfiguration() .failureRateThreshold(50) .waitDurationInOpenState(10000) .minimumNumberOfCalls(10) // Minimum calls before calculating failure rate .end() .log(\"Processing order: ${body}\") .to(\"http://order-service/api/orders\") .to(\"direct:payment-processor\") .onFallback() .log(LoggingLevel.ERROR, \"Circuit Breaker triggered for order processing\") .to(\"kafka:failed-orders\") .end(); // Payment Processing Route with Circuit Breaker from(\"direct:payment-processor\") .routeId(\"payment-processing-route\") .circuitBreaker() .resilience4jConfiguration() .failureRateThreshold(50) .waitDurationInOpenState(10000) .end() .log(\"Processing payment for order\") .to(\"http://payment-service/api/process\") .onFallback() .log(LoggingLevel.ERROR, \"Circuit Breaker triggered for payment processing\") .to(\"kafka:failed-payments\") .end(); // Error Handler Route from(\"direct:errorHandler\") .routeId(\"error-handler-route\") .log(LoggingLevel.ERROR, \"Error processing message: ${exception.message}\") .choice() .when(header(\"routeId\").isEqualTo(\"order-processing-route\")) .to(\"kafka:failed-orders\") .when(header(\"routeId\").isEqualTo(\"payment-processing-route\")) .to(\"kafka:failed-payments\") .otherwise() .to(\"kafka:dead-letter-queue\"); // Recovery Route - Processes failed messages when system recovers from(\"kafka:failed-orders\") .routeId(\"recovery-route\") .circuitBreaker() .resilience4jConfiguration() .failureRateThreshold(50) .waitDurationInOpenState(30000) .end() .log(\"Attempting to recover failed order\") .to(\"http://order-service/api/orders\") .onFallback() .log(LoggingLevel.ERROR, \"Recovery still not possible\") .end(); }}Let’s visualize the route structure and circuit breaker flow:flowchart TD subgraph \"Route Monitor\" M[Monitor Timer] --&gt; HC{Health Check} HC --&gt;|Healthy| SR[Start Routes] HC --&gt;|Unhealthy| SP[Stop Routes] end subgraph \"Order Processing\" KO[Kafka Orders] --&gt; CB1{Circuit Breaker} CB1 --&gt;|Closed| OP[Process Order] CB1 --&gt;|Open| FB1[Fallback - Failed Orders] OP --&gt; PP[Payment Processing] end subgraph \"Payment Processing\" PP --&gt; CB2{Circuit Breaker} CB2 --&gt;|Closed| PS[Process Payment] CB2 --&gt;|Open| FB2[Fallback - Failed Payments] end subgraph \"Recovery\" FB1 --&gt; RR[Recovery Route] FB2 --&gt; RR RR --&gt; CB3{Circuit Breaker} CB3 --&gt;|Closed| RPR[Reprocess] CB3 --&gt;|Open| FB3[Delay Recovery] end style CB1 fill:#f96,stroke:#333 style CB2 fill:#f96,stroke:#333 style CB3 fill:#f96,stroke:#333 style M fill:#69f,stroke:#333Key features of this implementation: Route Monitoring: Periodic health checks of destination systems Automatic route stopping/starting based on system health Configurable monitoring intervals Circuit Breaker Pattern: Uses Resilience4j for circuit breaker implementation Configurable failure thresholds and recovery times Separate circuit breakers for different routes Fallback mechanisms for handling failures Multiple Routes: Order processing route Payment processing route Error handling route Recovery route Error Handling: Dead letter channel for failed messages Different error queues for different types of failures Retry mechanism with exponential backoff Recovery Mechanism: Separate route for processing failed messages Circuit breaker protection for recovery attempts Longer wait times during recovery To use this in your application: Add dependencies to pom.xml:&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel&lt;/groupId&gt; &lt;artifactId&gt;camel-resilience4j&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.camel&lt;/groupId&gt; &lt;artifactId&gt;camel-kafka&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Configure in application.properties:# Circuit Breaker Configurationcamel.circuitbreaker.threshold=5camel.circuitbreaker.half-open-after=10000# Kafka Configurationkafka.bootstrap.servers=localhost:9092# Health Check Configurationhealth.check.interval=60000Health Check (Optional)In the previous example, we saw how based on a health check we can stop and start routes. The timer will invoke the health check route at fixed intervals. Let’s zoom into the implementation of the health check and see how it can be implemented to consider various different systems.import org.apache.camel.Exchange;import org.apache.camel.builder.RouteBuilder;import org.apache.http.client.config.RequestConfig;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClientBuilder;import org.apache.http.client.methods.HttpGet;import javax.sql.DataSource;import java.sql.Connection;import java.util.HashMap;import java.util.Map;import java.net.Socket;import lombok.extern.slf4j.Slf4j;@Slf4jpublic class HealthCheckRoute extends RouteBuilder { private static final int TIMEOUT = 5000; private static final String ORDER_SERVICE_URL = \"http://order-service/health\"; private static final String PAYMENT_SERVICE_URL = \"http://payment-service/health\"; private static final String MQ_HOST = \"localhost\"; private static final int MQ_PORT = 61616; private final DataSource dataSource; public HealthCheckRoute(DataSource dataSource) { this.dataSource = dataSource; } @Override public void configure() throws Exception { from(\"timer:routeMonitor?period=60000\") .routeId(\"health-check-route\") .process(this::checkDestinationHealth) .choice() .when(header(\"systemStatus\").isEqualTo(\"DOWN\")) .log(\"System status is DOWN. Stopping routes...\") .process(exchange -&gt; stopAffectedRoutes(exchange)) .otherwise() .log(\"System status is UP. Starting routes if needed...\") .process(exchange -&gt; startRoutes(exchange)); } private void checkDestinationHealth(Exchange exchange) { Map&lt;String, ServiceHealth&gt; healthResults = new HashMap&lt;&gt;(); // Check each service and store results healthResults.put(\"orderService\", checkHttpService(ORDER_SERVICE_URL)); healthResults.put(\"paymentService\", checkHttpService(PAYMENT_SERVICE_URL)); healthResults.put(\"database\", checkDatabase()); healthResults.put(\"messageQueue\", checkMessageQueue()); // Determine overall system status boolean isSystemHealthy = healthResults.values().stream() .allMatch(ServiceHealth::isHealthy); // Set headers with detailed health information exchange.getIn().setHeader(\"systemStatus\", isSystemHealthy ? \"UP\" : \"DOWN\"); exchange.getIn().setHeader(\"healthResults\", healthResults); // Store specific service statuses for granular route control exchange.getIn().setHeader(\"orderServiceStatus\", healthResults.get(\"orderService\").isHealthy() ? \"UP\" : \"DOWN\"); exchange.getIn().setHeader(\"paymentServiceStatus\", healthResults.get(\"paymentService\").isHealthy() ? \"UP\" : \"DOWN\"); } private ServiceHealth checkHttpService(String url) { try (CloseableHttpClient client = createHttpClient()) { HttpGet request = new HttpGet(url); int responseCode = client.execute(request) .getStatusLine() .getStatusCode(); boolean isHealthy = responseCode &gt;= 200 &amp;&amp; responseCode &lt; 300; return new ServiceHealth( isHealthy, isHealthy ? \"Service responding normally\" : \"Service returned status code: \" + responseCode ); } catch (Exception e) { log.error(\"Error checking HTTP service {}: {}\", url, e.getMessage()); return new ServiceHealth(false, \"Service check failed: \" + e.getMessage()); } } private ServiceHealth checkDatabase() { try (Connection conn = dataSource.getConnection()) { boolean isValid = conn.isValid(TIMEOUT); return new ServiceHealth( isValid, isValid ? \"Database connection is valid\" : \"Database connection test failed\" ); } catch (Exception e) { log.error(\"Database health check failed: {}\", e.getMessage()); return new ServiceHealth(false, \"Database check failed: \" + e.getMessage()); } } private ServiceHealth checkMessageQueue() { try (Socket socket = new Socket(MQ_HOST, MQ_PORT)) { boolean isConnected = socket.isConnected(); return new ServiceHealth( isConnected, isConnected ? \"Message queue is accessible\" : \"Could not connect to message queue\" ); } catch (Exception e) { log.error(\"Message queue health check failed: {}\", e.getMessage()); return new ServiceHealth(false, \"Message queue check failed: \" + e.getMessage()); } } private CloseableHttpClient createHttpClient() { RequestConfig config = RequestConfig.custom() .setConnectTimeout(TIMEOUT) .setConnectionRequestTimeout(TIMEOUT) .setSocketTimeout(TIMEOUT) .build(); return HttpClientBuilder.create() .setDefaultRequestConfig(config) .build(); } private void stopAffectedRoutes(Exchange exchange) { Map&lt;String, ServiceHealth&gt; healthResults = exchange.getIn().getHeader(\"healthResults\", Map.class); try { if (!healthResults.get(\"orderService\").isHealthy()) { getContext().getRouteController().stopRoute(\"order-processing-route\"); log.info(\"Stopped order-processing-route due to health check failure\"); } if (!healthResults.get(\"paymentService\").isHealthy()) { getContext().getRouteController().stopRoute(\"payment-processing-route\"); log.info(\"Stopped payment-processing-route due to health check failure\"); } } catch (Exception e) { log.error(\"Error stopping routes: {}\", e.getMessage()); } } private void startRoutes(Exchange exchange) { Map&lt;String, ServiceHealth&gt; healthResults = exchange.getIn().getHeader(\"healthResults\", Map.class); try { if (healthResults.get(\"orderService\").isHealthy() &amp;&amp; !getContext().getRouteController() .getRouteStatus(\"order-processing-route\").isStarted()) { getContext().getRouteController().startRoute(\"order-processing-route\"); log.info(\"Started order-processing-route after health check recovery\"); } if (healthResults.get(\"paymentService\").isHealthy() &amp;&amp; !getContext().getRouteController() .getRouteStatus(\"payment-processing-route\").isStarted()) { getContext().getRouteController().startRoute(\"payment-processing-route\"); log.info(\"Started payment-processing-route after health check recovery\"); } } catch (Exception e) { log.error(\"Error starting routes: {}\", e.getMessage()); } }}@lombok.Valueclass ServiceHealth { boolean healthy; String message;}Health Check Visualizationflowchart TD Timer[Timer Trigger] --&gt; HC[Health Check Process] subgraph \"Health Checks\" HC --&gt; |Check| HTTP[HTTP Services] HC --&gt; |Check| DB[Database] HC --&gt; |Check| MQ[Message Queue] end subgraph \"Status Evaluation\" HTTP --&gt; Eval[Evaluate Results] DB --&gt; Eval MQ --&gt; Eval Eval --&gt; Status{System Status} end Status --&gt;|DOWN| Stop[Stop Affected Routes] Status --&gt;|UP| Start[Start Routes if Stopped] style HC fill:#f96,stroke:#333 style Status fill:#69f,stroke:#333Key points about the implementation: Health Check Process: Checks multiple services: HTTP endpoints, database, message queue Each check has a timeout (5 seconds) Results are stored in a ServiceHealth object containing: healthy: boolean status message: detailed status message Status Headers: systemStatus: Overall system status (“UP” or “DOWN”) healthResults: Map containing detailed health check results orderServiceStatus: Specific status for order service paymentServiceStatus: Specific status for payment service Route Control: Routes are stopped/started based on their specific service health Order processing route depends on order service health Payment processing route depends on payment service health To use this implementation: Add required dependencies:&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Configure in application.properties:# Health Check Configurationhealth.check.timeout=5000health.check.order.service.url=http://order-service/healthhealth.check.payment.service.url=http://payment-service/healthhealth.check.mq.host=localhosthealth.check.mq.port=61616ConclusionApache Camel provides a powerful framework for implementing enterprise integration patterns. By combining these patterns with robust system management features like circuit breakers and health checks, you can build reliable, scalable integration solutions.Remember to always: Implement proper error handling Monitor system health Use circuit breakers to prevent cascade failures Maintain clear logging and monitoring Consider message persistence and reliability requirementsFor more information, visit the Apache Camel documentation." }, { "title": "Tech Blogs: Become better software Engineer", "url": "/posts/tech-blogs-become-better-software-engineer/", "categories": "Blogging, Article", "tags": "softwareengineering", "date": "2025-01-20 00:00:00 +0530", "snippet": "Tech Company Blogs Netflix Tech Blog Google Developers Blog Amazon Web Services (AWS) Blog Uber Engineering Blog Stripe Engineering Blog Airbnb Engineering Blog Meta Engineering Blog Microsoft Developer Blog Pinterest Engineering Blog LinkedIn Engineering Blog Spotify Engineering Blog Twitter Engineering Blog Slack Engineering Blog Cloud Engineering Blog by Dropbox Instacart Engineering Blog DoorDash Engineering Blog Shopify Engineering Blog Etsy Engineering Blog (Code as Craft) TikTok/ByteDance Engineering Blog Robinhood Engineering Blog Datadog Engineering Blog WePay Engineering Blog Zendesk Engineering Blog Grammarly Engineering Blog Square Engineering Blog Adobe Tech Blog Cloudflare Blog Notion Engineering Blog Figma Engineering Blog Canva Engineering Blog Robin Engineering Blog Monzo Engineering Blog Asana Engineering Blog Atlassian Engineering Blog Okta Developer BlogCommunity and Open Source Blogs GitHub Engineering Blog HashiCorp Engineering Blog Reddit Engineering Blog Mozilla Hacks Dev.to ThoughtWorks Insights DigitalOcean Community Hacker Noon Medium Engineering FreeCodeCamp Blog Patreon Engineering Blog Stack Overflow Blog Code Better Open Source Initiative Blog Apache Software Foundation Blog Linux Foundation Blog OpenTelemetry Blog GitLab BlogIndependent and Developer-Centric Blogs Martin Fowler’s Blog High Scalability Coding Horror Kent Beck’s Blog Joel on Software Scott Hanselman’s Blog Bartosz Milewski’s Blog Paul Graham’s Essays Overreacted by Dan Abramov Ben Eater Patrick McKenzie (Patio11) Brendan Gregg’s Blog Troy Hunt’s BlogResearch-Oriented Blogs DeepMind Blog OpenAI Blog NVIDIA Developer Blog Berkeley AI Research (BAIR) Blog Distill Google AI Blog MIT Technology Review Stanford HAI Blog Microsoft Research Blog Carnegie Mellon Software BlogTech Magazine and News Blogs InfoQ DZone Smashing Magazine Ars Technica LWN.net (Linux News) Better Programming Turing Blog Dark Reading KDnuggetsSpecialized Engineering Blogs JetBrains Blog Cloud Native Computing Foundation (CNCF) Blog Elastic Blog NGINX Blog Kubernetes Blog Serverless Blog Terraform Blog Jenkins Blog Apache Kafka Blog Postman Engineering Blog PyTorch Blog Rust Blog OWASP Blog Apache Spark Blog dbt Blog Looker Developer BlogData Engineering and Analytics Blogs Cloudera Engineering Blog Snowflake Blog Mode Blog DataCamp Blog Towards Data ScienceCloud and Infrastructure Blogs AWS Architecture Blog Linode Developer Blog Microsoft Azure Blog VMware Blogs New Relic Blog CircleCI Blog Helm BlogProgramming Language-Specific Blogs Go Blog (Golang) Python Insider TypeScript Blog Elixir Blog C++ BlogSecurity-Focused Blogs Krebs on Security Bruce Schneier’s Blog Google Security Blog" }, { "title": "Understanding Aspect-Oriented Programming (AOP) in Spring Boot", "url": "/posts/spring-aop/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment", "date": "2024-12-10 00:00:00 +0530", "snippet": "IntroductionAspect-Oriented Programming (AOP) in Spring Boot allows developers to handle cross-cutting concerns (common code) such as logging, authentication, and validation in a modular way. AOP helps keep the main business logic clean by separating these concerns into reusable components called aspects.In this article, we will implement AOP in a Spring Boot application for a Users and Orders API. We will define aspects to: Validate incoming requests. Log invalid requests to the database. Log successful request and response details.As part of auditing, these three steps are common in applications that are expected to keep track of incoming requests and outgoing responses for specific or all important API calls.Key AOP ConceptsBefore diving into implementation, let’s briefly cover some key AOP concepts: Aspect: A class containing cross-cutting concerns. Advice: The action taken by an aspect at a specific point (e.g., before, after, or around method execution). Join Point: A specific point in the execution flow of the application (e.g., method execution). Pointcut: A predicate that matches join points, defining where advice should be applied. Custom Annotation: A user-defined annotation that can be used as a pointcut marker for AOP.Implementing AOP in a Spring Bootflowchart TD A[Client Request] --&gt; B{AOP Proxy} B --&gt;|Around Advice| C{Input Validation} C --&gt;|Valid| D[Business Logic] C --&gt;|Invalid| H[Validation Error] D --&gt; E[Response Processing] E --&gt; F[Logging] F --&gt; G[Client Response] H --&gt; G style B fill:#f9f,stroke:#333,stroke-width:4px style E fill:#bbf,stroke:#333,stroke-width:2pxStep 1: Define a Custom Annotation for Validate And Logging request and response@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface ValidateRequest {}This code defines a custom annotation @ValidateRequest in Java. The @Target(ElementType.METHOD) specifies that this annotation can only be applied to methods.The @Retention(RetentionPolicy.RUNTIME) indicates that the annotation will be available at runtime, allowing it to be accessed through reflection.Step 2: Create Users and Orders API Controllers@RestController@RequestMapping(value = \"/api/orders\", produces = MediaType.APPLICATION_JSON_VALUE)@RequiredArgsConstructorpublic class OrderController { private final OrderService orderService; @GetMapping public ResponseEntity&lt;List&lt;OrderDto&gt;&gt; getAllOrders() { return ResponseEntity.ok(orderService.findAll()); } @PostMapping @ValidateRequest public ResponseEntity&lt;Long&gt; createOrder(@RequestBody final OrderDto orderDTO) { return new ResponseEntity&lt;&gt;(orderService.create(orderDTO), HttpStatus.CREATED); }}@RestController@RequestMapping(value = \"/api/users\", produces = MediaType.APPLICATION_JSON_VALUE)@RequiredArgsConstructorpublic class UserController { private final UserService userService; @GetMapping public ResponseEntity&lt;List&lt;UserDto&gt;&gt; getAllUsers() { return ResponseEntity.ok(userService.findAll()); } @PostMapping @ValidateRequest public ResponseEntity&lt;Long&gt; createUser(@RequestBody final UserDto userDto) { return new ResponseEntity&lt;&gt;(userService.create(userDto), HttpStatus.CREATED); }}Step 3: Define an Aspect for validating and auditing Annotation-Based Pointcuts@Aspect@Component@RequiredArgsConstructorpublic class RequestValidationAndAuditingAspect { private final AuditRepository auditRepository; private final HttpServletRequest requestServlet; private final Validator validator; @Pointcut(\"@annotation(dev.pravin.ecombackend.annotations.ValidateRequest)\") public void validateRequestPointcut() {} @Around(\"validateRequestPointcut() &amp;&amp; args(request,..)\") public Object validateAndAuditRequest(ProceedingJoinPoint joinPoint, Object request) throws Throwable { validateRequest(request); Object response = joinPoint.proceed(); auditDetails(request, response); return response; } private void auditDetails(Object request, Object response) { String correlationId = this.requestServlet.getHeader(\"Correlation-Id\"); String requestDetails = request.toString(); ObjectMapper objectMapper = new ObjectMapper(); String responseJson = objectMapper.writeValueAsString(response) String requestJson = objectMapper.writeValueAsString(request) logAudit(correlationId, requestJson, responseJson); } private void validateRequest(Object request) { Set&lt;ConstraintViolation&lt;Object&gt;&gt; violations = validator.validate(request); if (!violations.isEmpty()) { logBadRequest(request, violations); throw new BadRequestException(\"Invalid request parameters.\"); } } private void logBadRequest(Object request, Set&lt;ConstraintViolation&lt;Object&gt;&gt; violations) { String violationDetails = violations.toString(); String correlationId = this.requestServlet.getHeader(\"Correlation-Id\"); auditRepository.save(AuditLog.builder().status(\"BAD_REQUEST\") .requestDetails(correlationId + \"--\" + request.toString()) .responseDetails(violationDetails) .build()); } private void logAudit(String correlationId, String requestDetails, Object response) { auditRepository.save(AuditLog.builder().status(\"SUCCESS\") .requestDetails(correlationId + \"--\" + requestDetails) .responseDetails(response.toString()) .build()); }}This code defines an aspect RequestValidationAndAuditingAspect for validating and auditing requests and responses in a Spring Boot application. The @Aspect annotation indicates that this class is an aspect. The @Component annotation makes it a Spring-managed bean. The @RequiredArgsConstructor annotation generates a constructor with required arguments.The aspect contains: A pointcut validateRequestPointcut that matches methods annotated with @ValidateRequest. An around advice validateAndAuditRequest that: Validates the request using validateRequest. Proceeds with the method execution. Audits the request and response using auditDetails. The auditDetails method logs the request and response details. The validateRequest method validates the request and logs bad requests if there are validation errors. The logBadRequest and logAudit methods save audit logs to the database.Step 4: Create a Database Entity@Builder@Entity@AllArgsConstructorpublic class AuditLog { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String status; private String requestDetails; private String responseDetails;}Step 5: Define the Repository for Logging Requests@Repositorypublic interface AuditRepository extends JpaRepository&lt;AuditLog, Long&gt; {}let’s visualize the AOP concept and how aspects interact with business logic:stateDiagram-v2 [*] --&gt; RequestReceived RequestReceived --&gt; Validation: @ValidateRequest Validation --&gt; MethodExecution: Valid Validation --&gt; ErrorLogging: Invalid MethodExecution --&gt; ResponseProcessing: Success MethodExecution --&gt; ErrorLogging: Exception ResponseProcessing --&gt; AuditLogging ErrorLogging --&gt; AuditLogging AuditLogging --&gt; [*] note right of Validation Checks: - Required fields - Data format - Business rules end note note right of AuditLogging Stores: - Request details - Response/Error - Correlation ID - Timestamp end noteExplanation of AOP Usage Join Point: Any execution of a method in OrderController and UserController. Pointcut: Defined by @Pointcut(\"@annotation(dev.pravin.ecombackend.annotations.ValidateRequest)\") to apply advice for annotated methods in OrderController and UserController.Types of AdviceBefore Advice Description: This advice runs before the method execution. It is used to perform actions such as logging or security checks before the actual method logic is executed. Example: Validates the token in the API request for all RestController methods.After Advice Description: This advice runs after the method execution, regardless of its outcome. It is useful for cleanup actions or logging that should happen whether the method succeeds or fails. Example: Logs a message after the method execution.AfterReturning Advice Description: This advice runs after the method successfully returns a result. It can be used to log the return value or perform actions based on the result. Example: Tracks successful API calls as part of observability.AfterThrowing Advice Description: This advice runs if the method throws an exception. It is useful for logging errors or performing actions in response to an error. Example: Tracks failed API calls and logs the HTTP status code.Around Advice Description: This advice runs before and after the method execution. It can control whether the method executes at all, and can modify the return value or throw an exception. Example: Validates the request and audits the request and response.AOP Advice Execution SequencesequenceDiagram participant C as Client participant BA as Before Advice participant M as Method participant AA as After Advice participant AR as AfterReturning participant AT as AfterThrowing C-&gt;&gt;BA: Request Note over BA: @Before executes BA-&gt;&gt;M: Continue if validation passes alt Successful Execution M-&gt;&gt;AR: Method completes AR-&gt;&gt;AA: Success path Note over AR: @AfterReturning executes else Exception Thrown M-&gt;&gt;AT: Method throws exception AT-&gt;&gt;AA: Error path Note over AT: @AfterThrowing executes end Note over AA: @After executes AA-&gt;&gt;C: ResponseMore Examples for Different Types of AdviceBefore AdviceThis advice runs before the method execution. Let’s take a simple example that validates the token in the API request for all RestController methods.@Service@RequiredArgsConstructorpublic class TokenValidationService { private final HttpServletRequest requestServlet; public boolean validateToken(String authToken) { if (authToken == null || !isValid(authToken)) { return false; } return true; } private boolean isValid(String token) { // token validation logic return true; // Return true if valid, false otherwise }}@Aspect@Component@RequiredArgsConstructorpublic class TokenValidationAspect { private final TokenValidationService tokenValidationService; @Pointcut(\"within(@org.springframework.web.bind.annotation.RestController *)\") public void controllerMethods() {} @Before(\"controllerMethods()\") public void validateToken(JoinPoint joinPoint) { if (!tokenValidationService.validateToken()) { throw new UnauthorizedException(\"Invalid or missing authentication token.\"); } }}After AdviceThis advice runs after the method execution, regardless of its outcome.@After(\"validateRequestPointcut()\")public void logAfter(JoinPoint joinPoint) { System.out.println(\"After method: \" + joinPoint.getSignature().getName());}AfterReturning And AfterThrowing AdviceAfterReturning advice runs after the method successfully returns a result, and AfterThrowing advice runs if the method throws an exception. Let’s take an example of tracking the API call as part of observability.&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-core&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.micrometer&lt;/groupId&gt; &lt;artifactId&gt;micrometer-registry-prometheus&lt;/artifactId&gt;&lt;/dependency&gt;@Aspect@Component@RequiredArgsConstructorpublic class ApiHealthAspect { private final MeterRegistry meterRegistry; @Pointcut(\"within(@org.springframework.web.bind.annotation.RestController *)\") public void controllerMethods() {} @AfterReturning(pointcut = \"controllerMethods()\", returning = \"result\") public void logAfterReturning(JoinPoint joinPoint, Object result) { String apiName = joinPoint.getSignature().getName(); meterRegistry.counter(\"api.success\", \"api\", apiName).increment(); } @AfterThrowing(pointcut = \"controllerMethods()\", throwing = \"error\") public void logAfterThrowing(JoinPoint joinPoint, Throwable error) { String apiName = joinPoint.getSignature().getName(); meterRegistry.counter(\"api.failure\", \"api\", apiName, \"status\", getStatus(error)).increment(); } private String getStatus(Throwable error) { if (error instanceof BadRequestException) { return \"4xx\"; } else { return \"5xx\"; } }}Comparison: Annotation-Based vs. Regular Expression-Based Pointcuts Criteria Annotation-Based (@ValidateRequest) Regex-Based (execution(..)) Granularity More flexible, can be applied selectively Matches all methods matching the pattern Code Intrusiveness Requires modifying methods with annotations No modifications needed in the target methods Maintainability Easier to manage since it explicitly marks important methods Might require frequent updates if method signatures change Performance Slightly better as only annotated methods are intercepted Can be less efficient if applied broadly Use Case Best for selective logging where explicit annotation is preferred Best for applying to multiple methods with a common pattern Aspect Ordering with @OrderIn Spring AOP, you can control the order in which aspects are applied using the @Order annotation. This is useful when you have multiple aspects that should be applied in a specific sequence.Example code@Aspect@Component@Order(1)public class FirstAspect { @Before(\"execution(* dev.pravin.ecombackend.service.*.*(..))\") public void beforeAdvice(JoinPoint joinPoint) { System.out.println(\"First Aspect - Before method: \" + joinPoint.getSignature().getName()); }}@Aspect@Component@Order(2)public class SecondAspect { @Before(\"execution(* dev.pravin.ecombackend.service.*.*(..))\") public void beforeAdvice(JoinPoint joinPoint) { System.out.println(\"Second Aspect - Before method: \" + joinPoint.getSignature().getName()); }}In this example, FirstAspect will be applied before SecondAspect due to the @Order annotation.AOP order SequencesequenceDiagram participant C as Client participant S as Security Aspect&lt;br&gt;@Order(1) participant V as Validation Aspect&lt;br&gt;@Order(2) participant L as Logging Aspect&lt;br&gt;@Order(3) participant M as Method C-&gt;&gt;S: Request Note over S: Security Check S-&gt;&gt;V: If authorized Note over V: Validate Input V-&gt;&gt;L: If valid Note over L: Log Request L-&gt;&gt;M: Execute method M-&gt;&gt;L: Response Note over L: Log Response L-&gt;&gt;V: Continue V-&gt;&gt;S: Continue S-&gt;&gt;C: Final ResponseWhen to Use Which? Use annotation-based pointcuts when you need explicit control over which methods should be intercepted. Use regex-based execution pointcuts when you want to apply AOP to multiple methods following a common pattern without modifying the source code.Best Practices and Design PatternsDesign Patterns Commonly Used with AOP Proxy Pattern: AOP uses the proxy pattern to create a proxy object that wraps the target object and intercepts method calls. Decorator Pattern: Similar to the proxy pattern, the decorator pattern adds additional behavior to objects dynamically. Chain of Responsibility: Multiple aspects can be applied in a chain, where each aspect handles a specific concern.Anti-Patterns to Avoid Overusing Aspects: Avoid using aspects for every cross-cutting concern. Use them judiciously to prevent code complexity. Tight Coupling: Ensure that aspects are loosely coupled with the business logic. Avoid direct dependencies between aspects and business classes. Performance Impact: Be mindful of the performance impact of aspects, especially if they are applied to frequently called methods.Guidelines for Aspect Design Single Responsibility: Each aspect should handle a single cross-cutting concern to promote modularity and maintainability. Reusability: Design aspects to be reusable across different parts of the application. Granularity: Use fine-grained pointcuts to apply aspects selectively and avoid broad application that can impact performance. Testing: Thoroughly test aspects to ensure they work as expected and do not introduce unintended side effects.ConclusionIn this article, we demonstrated how Aspect-Oriented Programming (AOP) in Spring Boot can handle cross-cutting concerns like request validation and auditing. By using custom annotations, pointcuts, and aspects, AOP keeps the business logic clean while modularizing tasks such as validation and logging.We also compared annotation-based and regex-based pointcuts. Annotation-based pointcuts offer precise control for selective interception, while regex-based pointcuts can apply AOP broadly.Overall, AOP helps improve code maintainability and separation of concerns in Spring Boot applications." }, { "title": "Kafka Schema Registry and JSON Schema: A Comprehensive Guide", "url": "/posts/kafka-schema-registry-and-json-schema-a-comprehensive-guide/", "categories": "Blogging, Article", "tags": "backenddevelopment, design", "date": "2024-11-08 00:00:00 +0530", "snippet": "Kafka Schema Registry and JSON Schema: A Comprehensive GuideIntroductionWhat is Kafka?Kafka is a distributed streaming platform designed for high throughput and low latency, widely used for: Event-driven architectures Real-time analytics Data pipelinesKafka Schema Registry: Core ConceptKafka Schema Registry is a centralized service that: Manages and validates message schemas in Kafka topics Provides schema versioning Ensures compatibility between producers and consumersThe Role of JSON SchemaJSON Schema is a declarative language for: Defining JSON data structures Enforcing data validation rules Ensuring consistent data across distributed systemsSchema Evolution and CompatibilityTypes of Schema CompatibilityCompatibility Types Explained Backward Compatibility: New schema versions can be read by consumers using older schemas Allows adding new fields Existing consumers can still process messages Forward Compatibility: Old schema versions can consume data from new schemas Allows removing fields carefully New consumers can process older message formats Full Compatibility: Combines backward and forward compatibility Provides maximum flexibility in schema evolution Schema Evolution ScenariosScenario 1: Making a Field NullableOne common schema evolution approach is making a field nullable. This allows the field to either contain a value or be absent (null).Initial Schema (Version 1):{ \"type\": \"object\", \"properties\": { \"user_id\": { \"type\": \"integer\" }, \"name\": { \"type\": \"string\" }, \"email\": { \"type\": \"string\" } }, \"required\": [\"user_id\", \"name\", \"email\"]}Evolved Schema (Version 2):{ \"type\": \"object\", \"properties\": { \"user_id\": { \"type\": \"integer\" }, \"name\": { \"type\": \"string\" }, \"email\": { \"type\": [\"string\", \"null\"] } }, \"required\": [\"user_id\", \"name\"]}In this evolution: The email field becomes optional email can now be a string or null Maintains compatibility with existing consumersScenario 2: Using Default ValuesDefault values provide a powerful mechanism for maintaining compatibility during schema changes, especially when making breaking changes like converting a nullable field to a required field.Schema with Default Value:{ \"type\": \"record\", \"name\": \"User\", \"fields\": [ { \"name\": \"fieldA\", \"type\": \"string\", \"default\": \"default_value\" } ]}Benefits of default values: Prevent serialization errors Maintain backward and forward compatibility Ensure smooth data flow during schema changesData Flow in Kafka with Schema RegistryData Flow Steps Producer Side: Retrieves or registers schema Serializes message using schema Sends message with schema ID to Kafka topic Kafka Topic: Stores messages with schema ID references Supports multiple schema versions Consumer Side: Reads message with schema ID Retrieves corresponding schema Deserializes message using schema Practical ImplementationDocker Compose Setupversion: '3'services: zookeeper: image: confluentinc/cp-zookeeper:latest container_name: zookeeper environment: ZOOKEEPER_CLIENT_PORT: 2181 ZOOKEEPER_TICK_TIME: 2000 ports: - \"2181:2181\" schema-registry: image: confluentinc/cp-schema-registry:7.8.0 hostname: schema-registry depends_on: - kafka-broker-1 ports: - \"8081:8081\" environment: SCHEMA_REGISTRY_HOST_NAME: schema-registry SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181' SCHEMA_REGISTRY_LISTENERS: http://schema-registry:8081 SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka-broker-1:9092,PLAINTEXT_INTERNAL://localhost:19092 SCHEMA_REGISTRY_DEBUG: 'true' kafka-broker-1: image: confluentinc/cp-kafka:latest hostname: kafka-broker-1 ports: - \"19092:19092\" depends_on: - zookeeper environment: KAFKA_BROKER_ID: 1 KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181' KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-1:9092,PLAINTEXT_INTERNAL://localhost:19092 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1Schema Registry API# Register a new schemacurl --request POST \\ --url http://localhost:8081/subjects/user-registration-test-value/versions \\ --header 'Content-Type: application/vnd.schemaregistry.v1+json' \\ --data '{\t\"schemaType\": \"JSON\",\t\"schema\": \"{ \\\"$schema\\\": \\\"http://json-schema.org/draft-07/schema#\\\", \\\"title\\\": \\\"User\\\", \\\"description\\\": \\\"Schema representing a user\\\", \\\"type\\\": \\\"object\\\", \\\"additionalProperties\\\": false, \\\"properties\\\": { \\\"name\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"Name of person.\\\" }, \\\"email\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"email of person.\\\" }, \\\"userId\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"user id in the system\\\" } }, \\\"required\\\": [\\\"name\\\", \\\"userId\\\", \\\"email\\\"] }\"}'# Fetch Schemacurl --request GET \\--url http://localhost:8081/subjects/user-registration-test-value/versions/latest# Delete Schemacurl --request DELETE \\ --url http://localhost:8081/subjects/user-registration-test-valueJava Integration ExamplesConfigurationplugins {\tid 'java'\tid 'org.springframework.boot' version '3.4.0'\tid 'io.spring.dependency-management' version '1.1.6'}group = 'dev.pravin'version = '0.0.1-SNAPSHOT'java {\ttoolchain {\t\tlanguageVersion = JavaLanguageVersion.of(21)\t}}repositories {\tmavenCentral()\tmaven {\t\turl \"https://packages.confluent.io/maven\"\t}}dependencies {\timplementation 'org.springframework.boot:spring-boot-starter'\timplementation 'org.springframework.boot:spring-boot-starter-web'\timplementation 'org.springframework.kafka:spring-kafka'\timplementation 'io.confluent:kafka-json-schema-serializer:7.8.0'\timplementation 'io.confluent:kafka-schema-registry-client:7.8.0'\timplementation 'org.projectlombok:lombok:1.18.36'\tannotationProcessor 'org.projectlombok:lombok:1.18.36'\ttestImplementation 'org.springframework.boot:spring-boot-starter-test'\ttestImplementation 'org.springframework.kafka:spring-kafka-test'\ttestRuntimeOnly 'org.junit.platform:junit-platform-launcher'}tasks.named('test') {\tuseJUnitPlatform()}spring.application.name=schema registry demoserver.port=8100spring.kafka.properties.use.latest.version=true@EnableKafka@Configurationpublic class KafkaConfig { public static final String BROKER_URL = \"localhost:19092\"; public static final String SCHEMA_REGISTRY_URL = \"http://localhost:8081\"; public static final String SPRING_KAFKA_ADMIN_CLIENT = \"spring-kafka-admin-client\"; public static final String GROUP_ID = \"group_id_23\"; @Bean public ProducerFactory&lt;String, UserRegistrationTest&gt; producerFactory() { Map&lt;String, Object&gt; config = new HashMap&lt;&gt;(); config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BROKER_URL); config.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); config.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaJsonSchemaSerializer.class); config.put(\"schema.registry.url\", SCHEMA_REGISTRY_URL); return new DefaultKafkaProducerFactory&lt;&gt;(config); } @Bean public KafkaTemplate&lt;String, UserRegistrationTest&gt; kafkaTemplate() { return new KafkaTemplate&lt;&gt;(producerFactory()); } @Bean public SchemaRegistryClient schemaRegistryClient() throws RestClientException, IOException { CachedSchemaRegistryClient client = new CachedSchemaRegistryClient(SCHEMA_REGISTRY_URL, 1); String jsonString = \"\"\" { \"$schema\": \"http://json-schema.org/draft-07/schema#\", \"title\": \"User\", \"description\": \"Schema representing a user\", \"type\": \"object\", \"additionalProperties\": false, \"properties\": { \"name\": { \"type\": \"string\", \"description\": \"Name of person.\" }, \"email\": { \"type\": \"string\", \"description\": \"email of person.\" }, \"userId\": { \"type\": \"string\", \"description\": \"user id in the system\" } }, \"required\": [\"name\", \"userId\", \"email\"] } \"\"\"; client.register(\"user-registration-test-value\", new JsonSchema(jsonString)); return client; } @Bean public ConsumerFactory&lt;String, UserRegistrationTest&gt; consumerFactory(SchemaRegistryClient schemaRegistryClient) { Map&lt;String, Object&gt; config = new HashMap&lt;&gt;(); config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BROKER_URL); config.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_ID); config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class); config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaJsonSchemaDeserializer.class); config.put(\"schema.registry.url\", SCHEMA_REGISTRY_URL); config.put(\"specific.json.reader\", true); return new DefaultKafkaConsumerFactory&lt;&gt;(config, new StringDeserializer(), new KafkaJsonSchemaDeserializer&lt;&gt;(schemaRegistryClient)); } @Bean public ConcurrentKafkaListenerContainerFactory&lt;String, UserRegistrationTest&gt; kafkaListenerContainerFactory(SchemaRegistryClient schemaRegistryClient) { ConcurrentKafkaListenerContainerFactory&lt;String, UserRegistrationTest&gt; factory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;(); factory.setConsumerFactory(consumerFactory(schemaRegistryClient)); return factory; }}Producer Example@Data@Builder@Getterpublic class UserRegistrationTest { private String userId; private String email; private String name;}@Service@RequiredArgsConstructorpublic class KafkaProducer { private final SchemaRegistryClient schemaRegistryClient; private final KafkaTemplate&lt;String, UserRegistrationTest&gt; kafkaTemplate; public void sendMessage(UserRegistrationTest userRegistrationTest) throws RestClientException, IOException { schemaRegistryClient.reset(); int version = schemaRegistryClient.getLatestSchemaMetadata(TOPIC+\"-value\").getVersion(); System.out.println(\"Version: \" + version); kafkaTemplate.send(TOPIC, userRegistrationTest); System.out.println(\"Message sent: \" + userRegistrationTest); }}@RestController@RequiredArgsConstructor@RequestMapping(\"/kafka\")public class KafkaController { private final KafkaProducer kafkaProducer; @PostMapping(\"/publish\") public String publishMessage(@RequestBody UserRegistrationTest userRegistrationTest) throws RestClientException, IOException { kafkaProducer.sendMessage(userRegistrationTest); return \"Message published: \" + userRegistrationTest.toString(); }}Consumer Example@Service@RequiredArgsConstructorpublic class KafkaConsumer { @KafkaListener(topics = TOPIC) public void consume(String message) { System.out.println(\"Message received: \" + message); }}Publish message to Kafka topiccurl --request POST \\ --url http://localhost:8100/kafka/publish \\ --header 'Content-Type: application/json' \\ --header 'User-Agent: insomnia/10.2.0' \\ --data '{\t\"userId\": \"pravin\",\t\"email\": \"pravin@pravin.dev\"}'Best Practices and ConsiderationsSchema Evolution Guidelines Minimize breaking changes Use default values for new required fields Test compatibility before deploying Monitor schema changesPotential Challenges Managing complex schema evolutions Ensuring backward/forward compatibility Performance overhead of schema validationConclusionKafka Schema Registry with JSON Schema provides a robust solution for managing data structures in distributed systems. By carefully implementing schema evolution strategies, developers can create flexible, maintainable, and scalable data streaming architectures.Key Takeaways Centralize schema management Implement careful evolution strategies Utilize default values Continuously test and validate schemasSample CodeDownload the code" }, { "title": "Understanding Spring @Transactional Annotation: A Comprehensive Guide", "url": "/posts/spring-transactional-annotation/", "categories": "Blogging, Article", "tags": "softwareengineering, backenddevelopment, spring", "date": "2024-10-01 00:00:00 +0530", "snippet": "Spring’s @Transactional annotation is a powerful tool for managing transactions in Java applications. It simplifies transaction management by allowing developers to define transactional behavior declaratively, eliminating the need for boilerplate code. In this blog post, we’ll explore how @Transactional works, its key attributes, common pitfalls, and best practices to help you master transaction management in Spring.Table of Contents How @Transactional Works Key Attributes of @Transactional Propagation Isolation Rollback Behavior Read-Only Transactions Timeout Common Pitfalls Self-Invocation Checked Exceptions Visibility Issues Best Practices ConclusionHow @Transactional WorksWhen a method is annotated with @Transactional, Spring creates a proxy that wraps the method call. This proxy manages the transaction boundaries: A new transaction is started before method execution (if required). The transaction is committed if the method executes successfully. The transaction is rolled back if an exception occurs (based on the rollback rules).Here’s a visual representation of how @Transactional works:sequenceDiagram participant Client participant Proxy participant TargetMethod participant TransactionManager Client-&gt;&gt;Proxy: Calls @Transactional method Proxy-&gt;&gt;TransactionManager: Begin Transaction TransactionManager--&gt;&gt;Proxy: Transaction Started Proxy-&gt;&gt;TargetMethod: Invoke Method TargetMethod--&gt;&gt;Proxy: Method Execution alt Successful Execution Proxy-&gt;&gt;TransactionManager: Commit Transaction TransactionManager--&gt;&gt;Proxy: Transaction Committed else Exception Occurs Proxy-&gt;&gt;TransactionManager: Rollback Transaction TransactionManager--&gt;&gt;Proxy: Transaction Rolled Back end Proxy--&gt;&gt;Client: Return Result or ExceptionKey Attributes of @TransactionalPropagationPropagation defines how transactions behave when a method is called within another transaction. It determines whether a new transaction should be created, an existing one should be used, or no transaction should be used at all.Propagation Types at a Glance Propagation Type Short Description REQUIRED (default) Uses an existing transaction if available; otherwise, starts a new one. REQUIRES_NEW Suspends the current transaction and starts a new one. MANDATORY Throws an exception if no existing transaction is found. SUPPORTS Runs within a transaction if one exists; otherwise, runs non-transactionally. NOT_SUPPORTED Runs the method outside of any transaction, suspending an existing one if necessary. NEVER Throws an exception if an active transaction exists. NESTED Executes within a nested transaction if an existing transaction is found. Detailed Breakdown with Examples REQUIRED Uses an existing transaction if available; otherwise, starts a new one. Example: @Transactional(propagation = Propagation.REQUIRED)public void methodA() { methodB(); // Reuses the same transaction if called within methodA} Explanation: If methodA is called without an existing transaction, a new transaction is started. If methodB is called within methodA, it reuses the same transaction. REQUIRES_NEW Suspends the current transaction and starts a new one. Example: @Transactional(propagation = Propagation.REQUIRES_NEW)public void methodB() { // Business logic in a new transaction} Explanation: If methodB is called within an existing transaction, the current transaction is suspended, and a new transaction is started for methodB. After methodB completes, the original transaction resumes. MANDATORY Throws an exception if no existing transaction is found. Example: @Transactional(propagation = Propagation.MANDATORY)public void methodC() { // Business logic that requires an existing transaction} Explanation: If methodC is called without an existing transaction, a TransactionRequiredException is thrown. SUPPORTS Runs within a transaction if one exists; otherwise, runs non-transactionally. Example: @Transactional(propagation = Propagation.SUPPORTS)public void methodD() { // Business logic that can run with or without a transaction} Explanation: If methodD is called within a transaction, it runs within that transaction. If no transaction exists, it runs without one. NOT_SUPPORTED Runs the method outside of any transaction, suspending an existing one if necessary. Example: @Transactional(propagation = Propagation.NOT_SUPPORTED)public void methodE() { // Business logic that should not run within a transaction} Explanation: If methodE is called within a transaction, the transaction is suspended, and the method runs without a transaction. After the method completes, the original transaction resumes. NEVER Throws an exception if an active transaction exists. Example: @Transactional(propagation = Propagation.NEVER)public void methodF() { // Business logic that must not run within a transaction} Explanation: If methodF is called within an active transaction, an IllegalTransactionStateException is thrown. NESTED Executes within a nested transaction if an existing transaction is found. Example: @Transactional(propagation = Propagation.NESTED)public void methodG() { // Business logic that runs in a nested transaction} Explanation: If methodG is called within an existing transaction, a nested transaction is created. If the outer transaction fails, the nested transaction is rolled back. If the nested transaction fails, the outer transaction can decide whether to commit or rollback. IsolationDefines how data modifications in one transaction are visible to others. Isolation Level Description DEFAULT Uses the database default isolation level. READ_UNCOMMITTED Allows dirty reads (reading uncommitted changes from other transactions). READ_COMMITTED Prevents dirty reads; a transaction sees only committed changes. REPEATABLE_READ Prevents non-repeatable reads; data read within a transaction remains consistent. SERIALIZABLE Ensures complete isolation but may cause performance overhead. Example:@Transactional(isolation = Isolation.REPEATABLE_READ)public void updateAccountBalance() { // Business logic to update balance}Rollback BehaviorSpecifies conditions under which transactions should be rolled back. Rollback Rule Description rollbackFor Defines exceptions that trigger rollback (e.g., rollbackFor = Exception.class). noRollbackFor Defines exceptions that should not trigger rollback. Example:@Transactional(rollbackFor = SQLException.class)public void saveData() throws SQLException { // Database operations that should roll back on SQL exception}Read-Only TransactionsIndicates that a transaction will not perform write operations, allowing optimizations.Example:@Transactional(readOnly = true)public List&lt;User&gt; getUsers() { return userRepository.findAll();}TimeoutSpecifies the maximum time (in seconds) a transaction can run before rollback.Example:@Transactional(timeout = 5)public void processLongRunningTask() { // Business logic}Common PitfallsSelf-InvocationTransactional methods calling each other within the same class do not trigger transactional behavior due to proxy-based AOP. This happens because the proxy is bypassed during self-invocation.Solution: Use AopContext.currentProxy() or refactor the code to call the method from another bean.@Servicepublic class MyService { public void outerMethod() { ((MyService) AopContext.currentProxy()).innerMethod(); } @Transactional public void innerMethod() { // Business logic }}Checked ExceptionsBy default, transactions roll back only on unchecked exceptions (RuntimeException or Error). Checked exceptions do not trigger rollback unless explicitly configured.Solution: Use the rollbackFor attribute to specify checked exceptions.@Transactional(rollbackFor = CustomCheckedException.class)public void myMethod() throws CustomCheckedException { // Business logic}Visibility IssuesThe method must be public to ensure Spring’s proxy mechanism applies correctly. Non-public methods (e.g., private, protected) will not be transactional.Solution: Always declare transactional methods as public.Best Practices Use @Transactional at the Service Layer: Apply @Transactional to service methods rather than DAO or repository methods. This ensures that business logic is executed within a single transaction. Avoid Long-Running Transactions: Use the timeout attribute to prevent transactions from holding database resources for too long. Leverage Read-Only Transactions: Mark read-only methods with readOnly = true to optimize database performance. Test Transactional Behavior: Write unit and integration tests to verify that transactions behave as expected, especially for rollback scenarios. Monitor Transaction Performance: Use tools like Spring Boot Actuator or database monitoring tools to track transaction performance and identify bottlenecks. ConclusionSpring’s @Transactional annotation provides a powerful way to manage transactions declaratively. By understanding attributes like propagation, isolation, rollback, read-only, and timeout, developers can fine-tune transaction management to optimize performance and consistency.Key Takeaways Use propagation to control how transactions interact. Choose the right isolation level for data consistency. Define rollback rules for specific exceptions. Mark read-only methods with readOnly = true for performance benefits. Be cautious of self-invocation and proxy limitations. Test and monitor transactional behavior to ensure reliability." }, { "title": "Understanding the impact of inaccurate User Acceptance Testing Environment", "url": "/posts/understanding-the-impact-of-inaccurate-user-acceptance-testing-environment/", "categories": "Blogging, Article", "tags": "backenddevelopment, design, uat", "date": "2024-01-22 00:00:00 +0530", "snippet": "User Acceptance Testing (UAT) is a crucial stage in the software development lifecycle. This process provides an opportunity for end-users to validate the functionality, usability, and compatibility of a system before it is officially launched or delivered. However, the UAT phase can often be marked by unexpected complications, with systems frequently exhibiting incorrect responses or incomplete functioning. These issues can lead to significant delays, increased developmental costs, and potential damage to the client’s reputation. In this article, we will delve deeper into understanding these challenges and explore effective strategies to address them.Identifying the ChallengesThe journey to the UAT phase often begins with a system that seems to function perfectly in the development environment. The software, in this stage, appears to meet all specified requirements and performs as expected. However, when the system is introduced to the UAT environment, the smooth sailing often turns into a stormy ride. The system may begin to return incorrect responses or demonstrate incomplete or inconsistent functionality. These unexpected issues can significantly prolong the project timeline, inflate development costs, and potentially harm the vendor’s reputation.Problems due to inaccurate setupLet me share my recent experience with one of the stock depository services (Let’s call them D).The D’s system plays a vital role in the financial market, and one of the products that I am using is designed to facilitate the pledging of shares. This process involves an investor pledging their shares to a lender as collateral for a loan. Once the pledge is created, the D’s system is supposed to send a callback response confirming the successful pledging of shares. However, issues arise due to the faulty state management, session management, security management design of the system in UAT. In Production environment, all works fine.generated using bing aiFaulty state management of the transaction in UATLet’s first start the discussion with state management of the transaction. It is of serious concern, if system is poorly handling state management. Lets me share one instance where I have faced problem due to this. In D’s system, every time a client’s customer wants to start the pledging flow, one transaction gets created which is used to identify the request in D’s internal system. Clients (Bank or NBFC) have to persist necessary details to query the state of the transaction at a later point in time to know progress as flow is performed on D’s portal and the only way to know is via callback response.A transaction can be in one of the following states, customer login to the web portal customer pledged the shares (using their Demat account) Depository Participants processed the shares for pledging Shares got transferred to Clients (Demat account)Failure response is not returned in status API, but callback response returns the status in UATTransaction status returned in callback response and transaction status API is not consistent. If some error happens, the callback response will provide the details of failure but let’s say if we call transaction status API, it will return the last successful status but not the failure details. This inconsistency forced clients to implement some state management at their end with additional logic. This is not good.Some cases cannot be tested in UAT for that requires a Prod account to testThere were some cases that I wanted to try out on the UAT environment which is supposed to be a replica of the prod environment, but when I heard that those cases I could not test in UAT, I asked question Why it cannot be tested in UAT. well, I didn’t get any proper response. The impact of this is I have to test in Prod and if all cases are successful then great, otherwise I have to fix it. This is just causing more delay for product release and causing inconvenience to the clients. If the system is well tested and proper testing is been employed with proper data, I don’t have to wait until Prod deployment to test the functionality. This simply explains the engineering practice by D’s tech team is bad and they have to do something. Since they have a good number of active Demat accounts, Banks and NBFCs have to work on some solutions to mitigate various problems. Also, It is not easy for newcomers to build a new depository service business as a barrier to entering this type of market is not easy, So seems like they don’t care now. But if other existing competitors come up with robust and better systems, D will be in trouble.Faulty session management in UATSession management is complex, specially if your user is allowed to login on multiple device. Let me share one instance where poor configuration caused trouble to my me. As per the documentation, D’s system should timeout the session if the transaction is in one of the below states, Got redirected the client’s app to D’s portal for login but no action for 10 mins. Post login, If the customer didn’t pledge shares within 15 min then the session will time out.In UAT, the second point cannot be tested and their team says some development is going on so it is disabled. This is a very bad management of the environment by D’s tech team. Ideally, they should not be using UAT as it is used by the external client to test their system and use that behavior as a reference for prod release. In one of the tests, I can go beyond 35 minutes in a single session post-login. There are various complex things clients could be doing and based on session timeout clients may want to restart the transaction as they want to lend the loan to the customer. This simply proves that D’s UAT is not in good shape. Due to the above reasons, my team has to rely on documentation to proceed with the code changes. It is frustrating as we cannot test our system using their UAT system. Also, the UAT system is not at all close to the production environment which gives less confidence to me whether my code will work properly or not. This is a kind of nightmare for anyone who is chasing a tight deadline.Faulty security management in UATThis is something no one wants to ignore being in the finance industry and you are building an app that your clients going to use for financial transactions.generated using bing aiA digital signature is asked for but not validatedDigital signature verification is a good security practice. Let me share an instance where inconsistency with this functionality in two different environments caused more confusion. According to the documentation, the server checks whether data has been tampered with for requests coming from the client. It creates a new digital signature of the data and compares it with the one sent by the client. But in UAT, this is not working, and the request works with a wrong digital signature. This is a serious problem as I don’t know whether the code changes added by me will work in production or not; And whether the digital signature generated is accepted by the server. If in production, the request fails due to an issue in digital signature it will again cause a delay to the product release.API contract asks for more details than needed to get transaction status.It is always a good practice to ask only necessary information to return data to the consumer of the API. In my case, The API that I am going to discuss has more details like digital signature, requestor name, requestor ID, transaction ID, and transaction initiation time.I found that, if the requestor name and transaction ID are provided then it could return the data. If only 2 information is requested, it is a waste to provide other information. The creation of a digital signature is computationally expensive, and if not required, it should be removed from the contract.One of the biggest risks that I see with this API is that, it works if we provide wrong data for digital signature, requestor ID, and transaction initiation time but correct data for requestor name and transaction ID. It exposes another brute force attack which could leak information for other clients in UAT.Navigating the Challengesgenerated using bing aiGiven the potential implications of these challenges, it’s crucial to navigate them effectively. One of the most effective strategies is to ensure that the UAT environment closely mirrors the actual production environment. This involves aligning the system configurations, using realistic test data, and simulating actual user behavior and load.Regular synchronization between the UAT and production environments can also play a critical role in the early detection and resolution of potential issues. By continually aligning these two environments, discrepancies can be identified and addressed before they escalate into larger problems.Another key part of navigating these challenges involves adopting a comprehensive and thorough testing strategy. This strategy should encompass various types of testing, including unit testing, integration testing, system testing, and acceptance testing, among others. Each phase of testing should be designed to identify specific types of issues, thereby enhancing the system’s overall robustness and reliability.ConclusionThe impact of inaccurate User Acceptance Testing (UAT) environments on software development projects, as illustrated through the experiences with a stock depository service (referred to as D), highlights significant challenges and potential consequences. The identified issues in the faulty state management, session management, and security checks during the UAT phase can lead to delays, increased development costs, and erode confidence in the system’s reliability.The challenges stem from discrepancies between the UAT and production environments, where unexpected behaviors arise in the UAT environment, jeopardizing the accuracy of testing outcomes. The issues with transaction status inconsistencies, untestable scenarios, session timeout discrepancies, and security vulnerabilities underscore the need for a more robust UAT environment.To address these challenges, it is crucial for organizations to ensure that their UAT environment closely mirrors the production environment. Regular synchronization, realistic test data, and simulation of actual user behavior can contribute to early issue detection and resolution. Additionally, adopting a comprehensive testing strategy that includes various testing phases is essential for enhancing the system’s robustness.The experiences shared serve as a reminder of the importance of maintaining the integrity of UAT processes. Successful navigation of these challenges requires a commitment to aligning environments, implementing thorough testing practices, and fostering a collaborative approach between development teams and end-users. Ultimately, a well-structured and accurate UAT environment is imperative for ensuring the successful and timely release of high-quality software products." }, { "title": "REST API Based Workflow Design Using iWF Framework", "url": "/posts/workflow-using-iwf-framework/", "categories": "Blogging, Article", "tags": "backenddevelopment, design, java, softwareengineering", "date": "2023-10-02 00:00:00 +0530", "snippet": "Using the iWF DSL framework to write workflow on the top of the Temporal platformIn this article, We are going to discuss Workflow and design a simple use case using the iWF framework (with Temporal Server).Part 1: Basics ConceptsBefore directly jumping to code, Let’s see some concepts about workflow and Temporal Server.Runtime platformProvide the ecosystem to run your applications and take care of the durability, availability, and scalability of the application. Both Cadence(from Uber) and Temporal share the same behavior as Temporal is forked from Cadence. Worker Processes are hosted by you and execute your code. The communication within the Cluster uses gRPC. Cadence/Temporal service is responsible for keeping workflow state and associated durable timers. It maintains internal queues (called task lists) which are used to dispatch tasks to external workers. Workflow execution is resumable, recoverable, and reactive. Cadence Doc Temporal Doc iWF ProjectTemporal System Overview for workflow executionWhat is Workflow?The term Workflow frequently denotes either a Workflow Type, a Workflow Definition, or a Workflow Execution. Workflows are sequences of tasks/steps that are executed in a specific order. It is based on the principle of separation of concerns. It focuses on the design and implementation of business processes as workflows. Workflow Definition: A Workflow Definition is the code that defines the constraints of a Workflow Execution. A Workflow Definition is often also referred to as a Workflow Function. Deterministic constraints: A critical aspect of developing Workflow Definitions is ensuring they exhibit certain deterministic traits – that is, making sure that the same Commands are emitted in the same sequence, whenever a corresponding Workflow Function Execution (instance of the Function Definition) is re-executed. Handling unreliable Worker Processes: Workflow Function Executions are completely oblivious to the Worker Process in terms of failures or downtime. Event Loop: Workflow execution states:What is a Workflow Engine? A workflow engine facilitates the flow of information, tasks, and events. The workflow engine is responsible for managing the execution of workflows. Workflow engines may also be referred to as Workflow Orchestration Engines The other components of the system are responsible for performing the specific tasks that make up the workflowsWhat is the activities or workflow state? An Activity is a normal function or method that executes a single, well-defined action (either short or long-running), such as calling another service, transcoding a media file, or sending an email message. Workflow code orchestrates the execution of Activities, persisting the results. If an Activity Function Execution fails, any future execution starts from the initial state Activity Functions are executed by Worker Processes Workflow State is used in the domain of the iWF framework which is the same as Activities in Cadence or Temporal.Event handlingWorkflows can be signaled about an external event. A signal is always point-to-point destined to a specific workflow instance. Signals are always processed in the order in which they are received. Human Tasks Process Execution Alteration SynchronizationExample: there is a requirement that all messages for a single user are processed sequentially but the underlying messaging infrastructure can deliver them in parallel. The Cadence solution would be to have a workflow per user and signal it when an event is received. Then the workflow would buffer all signals in an internal data structure and then call an activity for every signal received.Visibility View, Filter, and Search for Workflow Executions https://docs.temporal.io/visibility#list-filter-examples https://docs.temporal.io/visibility#search-attribute Query Workflow statePart 2: Temporal Server DesignBoth Cadence and Temporal provide a platform to execute our workflow function which is nothing but business logic.What are the components of the Cadence/Temporal server?The server consists of four independently scalable services: Frontend gateway: for rate limiting, routing, and authorizing. History service:maintains data (workflow mutable state, event and history storage, task queues, and timers). Matching service: hosts Task Queues for dispatching. Worker Service: Worker Service: for internal background Workflows (replication queue, system Workflows). Read more…Part 3: iWF Framework Design (Temporal as Backend)iWF is the framework that is developed to simply run the workflow and harness the full potential of the Cadence/Temporal Server.High-Level DesignAn iWF application is composed of several iWF workflow workers. These workers host REST APIs as “worker APIs” for the server to call. This callback pattern is similar to AWS Step Functions invoking Lambdas if you are familiar with it.An application also performs actions on workflow executions, such as starting, stopping, signaling, and retrieving results by calling iWF service APIs “service APIs”.The service APIs are provided by the “API service” in the iWF server. Internally, this API service communicates with the Cadence/Temporal service as its backend.Low-Level DesignUsers define their workflow code with a new SDK “iWF SDK” and the code is running in workers that talk to the iWF interpreter engine.The user workflow code defines a list of WorkflowState and kicks off a workflow execution. At any workflow state, the interpreter will call back the user workflow code to invoke some APIs (waitUntil or execute). Calling the waitUntil API will return some command requests. When the command requests are finished, the interpreter will then call the user workflow code to invoke the “execute” API to return a decision.The decision will decide how to complete or transition to other workflow states. At any API, workflow code can mutate the data/search attributes or publish to internal channels.RPC: Interact with workflow via APIUsing RPC annotation is one of the ways to interact with the workflow from external sources like REST API, and Kafka event. It can access persistence, internal channels, and state execution. RPC vs SignalBoth RPC and Signal are the two ways to communicate from an external system with the workflow execution. RPC is a synchronous API call - Definition The signal channel is an Asynchronous API.Some recommend, as a best practice, to use RPC with an Internal channel to asynchronously call the workflow. It is basically to replace the Signal API.RPC + Internal Channel =&gt; Signal Channel Internal-Channel and Signal Channel are both message queuesiWF Approach to Determinism and VersioningThere are some problems with the history replay for the workflow which causes non-determinism issues due to events like workflow state deletion or business logic changes, etc. iWF framework recommends using the flag to control the code execution as versioning is removed. Since there is no versioning, the non-determinism issue will not happen. Read more: IWF docExample of Atomicity using RPC for sending message, state transition, and saving data in DB.public class UserSignupWorkflow implements ObjectWorkflow { ... // Atomically read/write/send message in RPC @RPC public String verify(Context context, Persistence persistence, Communication communication) { String status = persistence.getDataAttribute(DA_Status, String.class); if (status.equals(\"verified\")) { return \"already verified\"; } persistence.setDataAttribute(DA_Status, \"verified\"); communication.publishInternalChannel(VERIFY_CHANNEL, null); return \"done\"; } ...}Part 4: Simple workflow example using iWFBelow is the workflow diagram of the KYC application based on Aadhaar.Step 1: Write Workflow definitionpublic class AadhaarKycWorkflow implements ObjectWorkflow { private final List&lt;StateDef&gt; stateDefs; public AadhaarKycWorkflow(Client client) { this.stateDefs = List.of( StateDef.startingState(new GenerateAadhaarOtpStep()), StateDef.nonStartingState(new ValidateAadhaarOtpStep()), StateDef.nonStartingState(new SaveAadhaarDetailsStep(client)) ); } @Override public List&lt;StateDef&gt; getWorkflowStates() { return stateDefs; } @Override public List&lt;PersistenceFieldDef&gt; getPersistenceSchema() { return List.of( SearchAttributeDef.create(SearchAttributeValueType.TEXT, \"customer_id\"), SearchAttributeDef.create(SearchAttributeValueType.TEXT, \"aadhaar_id\"), SearchAttributeDef.create(SearchAttributeValueType.TEXT, \"parentWorkflowId\") ); } @Override public List&lt;CommunicationMethodDef&gt; getCommunicationSchema() { return List.of( SignalChannelDef.create(String.class, \"AadhaarOtpSignal\"), SignalChannelDef.create(String.class, SC_SYSTEM_KYC_COMPLETED) ); }}StateDef.startingState: Starting step/task/activity which workflow will execute.StateDef.nonStartingState: It will be executed based on the State’s decision.getPersistenceSchema(): return types of data that will be accessed by the workflow. This data will be persisted as long as workflow history is preserved.getCommunicationSchema(): different types of communication that workflow will require to complete the tasks.Step 2: Write Workflow StateIt is also called the actual business rules that you want workflow to execute.public class ValidateAadhaarOtpStep implements WorkflowState&lt;String&gt; { @Override public Class&lt;String&gt; getInputType() { return String.class; } @Override public CommandRequest waitUntil(Context context, String input, Persistence persistence, Communication communication) { return CommandRequest.forAllCommandCompleted( SignalCommand.create(\"AadhaarOtpSignal\") ); } @Override public StateDecision execute(Context context, String aadhaarReferenceId, CommandResults commandResults, Persistence persistence, Communication communication) { var otp = (String) commandResults.getSignalValueByIndex(0); if (validateOtp(aadhaarReferenceId, otp)) { var details = fetchAadhaarDetails(aadhaarReferenceId, otp); return StateDecision.singleNextState(SaveAadhaarDetailsStep.class, details); } return StateDecision.singleNextState(ValidateAadhaarOtpStep.class, aadhaarReferenceId); } private Boolean validateOtp(String aadhaarReferenceId, String otp) { log.info(\"call aadhaar validate OTP API and fetch details for referenceId:{} and OTP:{}\", aadhaarReferenceId, otp); return Objects.equals(otp, \"1234\"); }}waitUntil() and execute(): are the two sub-steps that the workflow state executed in sequence to finish the task.waitUntil(): It returns Signals, Timer, or Internal event that the task is waiting to happen. Once that event is completed, execute() will be invoked.StateDecision: It returns the next state that workflow should be expected to execute. This will be executed only when the Temporal/Cadence Server schedules the task on the internal worker queue.Step 3: REST API endpoint to provide input to workflow @PostMapping(\"/kyc/aadhaar/otp\") ResponseEntity&lt;Response&gt; validateAadhaarOtp( @RequestParam String otp, @RequestParam String customerId) { var workflowId = getWorkflowIdForAadhaar(customerId); var response = client.describeWorkflow(workflowId); if (response.getWorkflowStatus().equals(WorkflowStatus.RUNNING)) { client.signalWorkflow(AadhaarKycWorkflow.class, workflowId, \"AadhaarOtpSignal\", otp); return ResponseEntity.ok(new Response(\"success\", \"\")); } return ResponseEntity.internalServerError().body(new Response(\"Workflow not running\", \"\")); } private String getWorkflowIdForAadhaar(String customerId) { return \"WF-LAMF-KYC-\"+customerId; }Part 5: Different Use CasesBelow are the examples to understand the usage of different APIs of the iWF framework. Microservice Orchestration user signup workflow KYC Workflow Product order workflow Loan application workflowProject Link: Github project iWF ProjectConclusioniWF framework has really simplified writing applications using workflow-oriented architecture. Writing applications with the direct APIs provided by Cadence/Temporal has a steep learning curve. Due to this, beginners make some common mistakes, and writing a workflow that uses the full potential of the system is challenging for newcomers.iWF Project is basically a wrapper on the top of Cadence and Temporal System which helps lower the learning curve and also helps writing workflow that uses the full potential of the system which is really great." }, { "title": "Code Smell Series: Big Class", "url": "/posts/code-smell-big-classs/", "categories": "Blogging, CodeSmellSeries", "tags": "coding, smells", "date": "2023-05-13 00:00:00 +0530", "snippet": " The secret to high-quality code is following the path of clean code.Is it a bad approach to have large code files?Yes, It’s bad to have big classes/files in programming. A big file means it is doing many things, and gets modified for multiple reasons. A file should have only one reason to change. It means it should be responsible for one thing.How to resolve the issue of big class?One of the techniques is to have the test in place for the target code for refactoring. If the previous developer has followed TDD, you already have the test code; Otherwise, write enough tests to cover the core functionality of the target code. Perform the refactoring to segregate the target code into different small files until each file serves a single purpose. Rerun the test. Once all test is green, we confidently say, the core functionality is preserved and refactoring is successful.Why is my refactoring taking more time?If there is a refactoring problem, it means your code is complex and has more dependencies on other objects. In the long run, creates an unexpected class that is dangerous to touch and hard to understand called God Class. God class means knowing more than it should know and is doing more than it should.Too much dependency causes code to take more space and time to instantiate. Refactoring takes more time, which indirectly means the target code has poor understandability. Here, most of the time spent in understanding the behavior and flow. It is not good, and refactoring method like method extraction and segregation could take more time than anticipated by the other developer.Below is an example of an implementation of a wrapper class that aggregates 3 API and expose a single API endpoint to be used by the requester to invoke 3 APIs in sequence.file name: LoanCreationWrapper.class(simplified)@Autowired ClientService clientService;@Autowired LoanAccountService loanAccountService;@Autowired LoanContractService loanContractService;public WrapperResponse createLoan (WrapperRequest wrapperRequest) {...}private void prepareWrapperResponse (WrapperResponse response, WrapperResponseBody responseBody) {...} private static LoanAccountCreation getSkippedLoanAccountResponse ( WrapperLoanAccountCreationRequest loanAccountData){...}private static LoanContractCreation getSkippedLoanContractResponse ( WrapperLoanContractCreationRequest loanContractData){...}private LoanContractCreation createLoanContract ( WrapperLoanContractCreationRequest loanContractData,String loanAccountId){...}private static LoanContractCreation getFailedIrrecoverableLoanContractCreationResponse( WrapperLoanContractCreationRequest loanContractData, String message) {...}private static LoanContractCreation getFailedRecoverableLoanContractCreationResponse ( WrapperLoanContractCreationRequest loanContractData, String message) {...}private static LoanContractCreation getSkippedLoanContractCreationResponse ( WrapperLoanContractCreationRequest loanContractData){...}private static LoanContractCreation getSuccessLoanContractCreationResponse ( WrapperLoanContractCreationRequest loanContractData,LoanContractCreationSuccessResponse loanContractCreationServiceResponse){...}private LoanContractCreationCallerRequest getLoanContractCreationCallerRequest ( WrapperLoanContractCreationRequest loanContractData,String loanAccountId ) throws DataNotFoundException,JsonProcessingException, InternalServerError{...}private LoanAccountCreation createLoanAccount ( WrapperLoanAccountCreationRequest loanAccountData,String customerId, String losLeadId) {...}private LoanAccountCreationRequest getLoanAccountCreationCallerRequest ( WrapperLoanAccountCreationRequest loanAccountData,String customerId, String losLeadId) throws DataNotFoundException, InternalServerError{...}private static LoanAccountCreation getSkippedLoanAccountCreationResponse ( WrapperLoanAccountCreationRequest loanAccountData){...}private static LoanAccountCreation getFailedRecoverableLoanAccountCreationResponse( WrapperLoanAccountCreationRequest loanAccountData,String message) {...}private static LoanAccountCreation getFailedIrrecoverableLoanAccountCreationResponse( WrapperLoanAccountCreationRequest loanAccountData, String message) {...}private static LoanAccountCreation getSuccessLoanAccountCreationResponse( WrapperLoanAccountCreationRequest loanAccountData, LoanAccountCreationSuccessResponse loanAccountCreationServiceResponse){...}private MandateDetails getMandateDetails( WrapperMandateDetails mandateDetails) {...}private SanctionLimit getSanctionLimit( WrapperSanctionLimit sanctionLimit) {...}private ClientCreditInformation getClientCreditInformation ( WrapperClientCreditInformation clientCreditInformation) throws DataNotFoundException, InternalServerError {...}private CreditBureauMilesData fetchCreditBureauData ( WrapperClientCreditInformation clientCreditInformation) throws DataNotFoundException, InternalServerError{...}private LoanAccountDetailsRequest getLoanAccountDetails ( WrapperLoanAccountDetails loanAccountDetails,String customerId, String losLeadId) throws DataNotFoundException, InternalServerError{...}private PaymentModeMilesData fetchPaymentModeData ( String paymentMode) throws DataNotFoundException, InternalServerError {...}private ClientCreation createClient (WrapperClientCreationRequest clientData) {...}private static ClientCreation getFailedRecoverableClientCreationResponse ( WrapperClientCreationRequest clientData, String message) {...}private static ClientCreation getSkippedClientCreationResponse ( WrapperClientCreationRequest clientData){...}private static ClientCreation getSkippedClientCreationResponse ( WrapperClientCreationRequest clientData){...}private static ClientCreation getSuccessClientCreationResponse ( WrapperClientCreationRequest clientData,ClientCreationSuccessResponse clientCreationServiceResponse){...}private static ClientCreation getFailedIrrecoverableClientCreationResponse ( WrapperClientCreationRequest clientData,String message) {...}private ClientCreationCallerRequest getClientCreationCallerRequest ( WrapperClientCreationRequest clientData) throws InternalServerError, JsonProcessingException, DataNotFoundException {...}private String fetchOccupationCode ( WrapperClientCreationRequest clientData) throws DataNotFoundException, InternalServerError{...}private CountryMilesData fetchCountryData ( WrapperClientCreationRequest clientData) throws DataNotFoundException, InternalServerError {...}private String fetchProfessionData ( WrapperClientCreationRequest clientData) throws DataNotFoundException, InternalServerError {...}private StateMilesData fetchStateData ( String stateAbbreviation) throws DataNotFoundException, InternalServerError {...}private List&lt;Bank&gt; getBankDetails ( List&lt;WrapperClientBankDetails&gt; clientBankDetails) throws JsonProcessingException,InternalServerError,DataNotFoundException {...}private Branch getBranchDetails ( WrapperClientBranchDetails clientBranchDetails) throws DataNotFoundException, InternalServerError{...}private BranchData fetchBranchData ( WrapperClientBranchDetails clientBranchDetails) throws DataNotFoundException, InternalServerError {...}private SubBroker getSubBrokerDetails ( WrapperClientSubBrokerDetails clientSubBrokerDetails) throws JsonProcessingException, InternalServerError, DataNotFoundException{...}private PrimaryRelationshipManager getPrimaryRMDetails ( WrapperClientPrimaryRM clientPrimarvRM) throws JsonProcessingException, InternalServerError, DataNotFoundException{...}private GST getGSTDetails (WrapperClientGSTDetails clientGSTDetails) throws DataNotFoundException. InternalServerError{...}As we can observe from the above code snippet, numerous responsibilities are assigned to the above file. Above file knows too many things about the dependency objects. If we see there is some grouping created with space separation to visually indicate that they are not related and serves different purpose based on functionality.Refactoring of the above code took more than half a day to understand and decide how to segregate the large class, which is knowing too much about the internal of 3 APIs in the smaller classes. Here methods are moved to LoanContractService, LoanAccountService, and ClientService respectively.Post refactoring above implementation looks like the below,file name: LoanCreationWrapper.class@Autowired ClientService clientService;@Autowired LoanAccountService loanAccountService;@Autowired LoanContractService loanContractService;public WrapperResponse createLoan (WrapperRequest wrapperRequest) {...}private void prepareWrapperResponse (WrapperResponse response, WrapperResponseBody responseBody) {...}file name: ClientService.class...private ClientCreation createClient (WrapperClientCreationRequest clientData) {...}private static ClientCreation getFailedRecoverableClientCreationResponse ( WrapperClientCreationRequest clientData, String message) {...}private static ClientCreation getSkippedClientCreationResponse ( WrapperClientCreationRequest clientData){...}private static ClientCreation getSkippedClientCreationResponse ( WrapperClientCreationRequest clientData){...}private static ClientCreation getSuccessClientCreationResponse ( WrapperClientCreationRequest clientData,ClientCreationSuccessResponse clientCreationServiceResponse){...}private static ClientCreation getFailedIrrecoverableClientCreationResponse ( WrapperClientCreationRequest clientData,String message) {...}private ClientCreationCallerRequest getClientCreationCallerRequest ( WrapperClientCreationRequest clientData) throws InternalServerError, JsonProcessingException, DataNotFoundException {...}private String fetchOccupationCode ( WrapperClientCreationRequest clientData) throws DataNotFoundException, InternalServerError{...}private CountryMilesData fetchCountryData ( WrapperClientCreationRequest clientData) throws DataNotFoundException, InternalServerError {...}private String fetchProfessionData ( WrapperClientCreationRequest clientData) throws DataNotFoundException, InternalServerError {...}private StateMilesData fetchStateData ( String stateAbbreviation) throws DataNotFoundException, InternalServerError {...}private List&lt;Bank&gt; getBankDetails ( List&lt;WrapperClientBankDetails&gt; clientBankDetails) throws JsonProcessingException,InternalServerError,DataNotFoundException {...}private Branch getBranchDetails ( WrapperClientBranchDetails clientBranchDetails) throws DataNotFoundException, InternalServerError{...}private BranchData fetchBranchData ( WrapperClientBranchDetails clientBranchDetails) throws DataNotFoundException, InternalServerError {...}private SubBroker getSubBrokerDetails ( WrapperClientSubBrokerDetails clientSubBrokerDetails) throws JsonProcessingException, InternalServerError, DataNotFoundException{...}private PrimaryRelationshipManager getPrimaryRMDetails ( WrapperClientPrimaryRM clientPrimarvRM) throws JsonProcessingException, InternalServerError, DataNotFoundException{...}private GST getGSTDetails (WrapperClientGSTDetails clientGSTDetails) throws DataNotFoundException. InternalServerError{...}file name: LoanAccountService.class...private LoanAccountCreation createLoanAccount ( WrapperLoanAccountCreationRequest loanAccountData,String customerId, String losLeadId) {...}private LoanAccountCreationRequest getLoanAccountCreationCallerRequest ( WrapperLoanAccountCreationRequest loanAccountData,String customerId, String losLeadId) throws DataNotFoundException, InternalServerError{...}private static LoanAccountCreation getSkippedLoanAccountCreationResponse ( WrapperLoanAccountCreationRequest loanAccountData){...}private static LoanAccountCreation getFailedRecoverableLoanAccountCreationResponse( WrapperLoanAccountCreationRequest loanAccountData,String message) {...}private static LoanAccountCreation getFailedIrrecoverableLoanAccountCreationResponse( WrapperLoanAccountCreationRequest loanAccountData, String message) {...}private static LoanAccountCreation getSuccessLoanAccountCreationResponse( WrapperLoanAccountCreationRequest loanAccountData, LoanAccountCreationSuccessResponse loanAccountCreationServiceResponse){...}private MandateDetails getMandateDetails( WrapperMandateDetails mandateDetails) {...}private SanctionLimit getSanctionLimit( WrapperSanctionLimit sanctionLimit) {...}private ClientCreditInformation getClientCreditInformation ( WrapperClientCreditInformation clientCreditInformation) throws DataNotFoundException, InternalServerError {...}private CreditBureauMilesData fetchCreditBureauData ( WrapperClientCreditInformation clientCreditInformation) throws DataNotFoundException, InternalServerError{...}private LoanAccountDetailsRequest getLoanAccountDetails ( WrapperLoanAccountDetails loanAccountDetails,String customerId, String losLeadId) throws DataNotFoundException, InternalServerError{...}private PaymentModeMilesData fetchPaymentModeData ( String paymentMode) throws DataNotFoundException, InternalServerError {...}private static LoanAccountCreation getSkippedLoanAccountResponse ( WrapperLoanAccountCreationRequest loanAccountData){...}file name: LoanContractService.class...private LoanContractCreation createLoanContract ( WrapperLoanContractCreationRequest loanContractData,String loanAccountId){...}private static LoanContractCreation getFailedIrrecoverableLoanContractCreationResponse( WrapperLoanContractCreationRequest loanContractData, String message) {...}private static LoanContractCreation getFailedRecoverableLoanContractCreationResponse ( WrapperLoanContractCreationRequest loanContractData, String message) {...}private static LoanContractCreation getSkippedLoanContractCreationResponse ( WrapperLoanContractCreationRequest loanContractData){...}private static LoanContractCreation getSuccessLoanContractCreationResponse ( WrapperLoanContractCreationRequest loanContractData,LoanContractCreationSuccessResponse loanContractCreationServiceResponse){...}private LoanContractCreationCallerRequest getLoanContractCreationCallerRequest ( WrapperLoanContractCreationRequest loanContractData,String loanAccountId ) throws DataNotFoundException,JsonProcessingException, InternalServerError{...}private static LoanContractCreation getSkippedLoanContractResponse ( WrapperLoanContractCreationRequest loanContractData){...}Structure of Wrapper response is,file name: WrapperResponse.classpublic class WrapperResponse {// other fields\tprivate WrapperResponseBody wrapperResponseBody;}public class WrapperResponseBody {\tprivate ClientResponse clientResponse;\tprivate LoanAccountResponse loanAccountResponse;\tprivate LoanContractResponse loanContractResponse;}In short, the wrapper should only invoke the API and collect the responses and return.Different refactoring techniques used to achieve the above results, Extract Method Move Method Remove Middle Man Push Down MethodConclusionWhen we dirty our hands while working with such code in a real project, it will not be that easy to identify patterns or grouping. It took me time to understand the code and later perform the refactoring. Refactoring could take additional time if the test of the target code is complex. I have experienced one instance, where I had to understand code and tests to perform the refactoring. Due to its complex implementation and testing, it took more time than expected." }, { "title": "Understaning RAFT - distributed consensus protocol", "url": "/posts/understanding-raft-distributed-consensus-protocol/", "categories": "Presentation, system-design", "tags": "design, backenddevelopment, softwareengineering", "date": "2023-05-07 00:00:00 +0530", "snippet": "RAFT - Distributed Consensus ProtocolIn this presentation, I talked about the RAFT consensus algorithm which is commonly used in large different systems to achieve consensus in a distributed environment. Many systems uses a custom implementation of the RAFT algorithm that is understandable and easy to implement compared to the Paxos algorithm for distributed consensus protocol.We will start from the basic understanding of the algorithm and later see an issue with this algorithm. We will also talk on recent incident that caused an internet outage for many people and businesses.Deck Diagramdownload and view svg file in separate viewer.Further readingsBlogs A Byzantine failure in the real world Consistent Core Generation Clock Emergent leader HeartbeatVideo: “Raft - The Understandable Distributed Protocol” by Ben Johnson (2013)" }, { "title": "Code Smell Series: Unit Test", "url": "/posts/code-smell-in-unit-test/", "categories": "Blogging, CodeSmellSeries", "tags": "coding, smells", "date": "2023-05-01 20:00:00 +0530", "snippet": " The secret to high-quality code is following the path of clean code.A code will remain at its highest quality if it has an understandable and meaningful test. Robert C. Martin in his book, “Clean Code: A Handbook of Agile Software Craftsmanship” mentioned a very nice acronym for clean code in unit testing.F.I.R.S.T.As per clean code practice, all tests follow the five rules below that form the F.I.R.S.T. acronym. The below explanation is taken from the Clean Code Book by Robert C. Martin,Fast: Tests should be fast. They should run quickly. When tests run slow, you won’t want to run them frequently. If you don’t run them frequently, you won’t find problems early enough to fix them easily. You won’t feel as free to clean up the code. Eventually, the code will begin to rot.Independent: Tests should not depend on each other. One test should not set up the conditions for the next test. You should be able to run each test independently and run the tests in any order you like. When tests depend on each other, then the first one to fail causes a cascade of downstream failures, making diagnosis difficult and hiding downstream defects.Repeatable: Tests should be repeatable in any environment. You should be able to run the tests in the production environment, in the QA environment, and on your local development machine without a network. If your tests aren’t repeatable in any environment, then you’ll always have an excuse for why they fail. You’ll also find yourself unable to run the tests when the environment isn’t available.Self-Validating: The tests should have a boolean output. Either they pass or fail. You should not have to read through a log file to tell whether the tests pass. You should not have to manually compare two different text files to see whether the tests pass. If the tests aren’t self-validating, then failure can become subjective, and running the tests can require a long manual evaluation.Timely: The tests need to be written in a timely fashion. Unit tests should be written just before the production code that makes them pass. If you write tests after the production code, then you may find the production code to be hard to test. You may decide that some production code is too hard to test. You may not design the production code to be testable.Rule 1: Single Concept per TestAs per this rule, a single test function should test a single thing/concept. We don’t want long test functions that go testing one miscellaneous thing after another. Below code snippet is an example of such a test. This test function should be segregated into three independent tests because it tests three independent things. Merging them all together into the same function forces the reader to figure out why each section is there and what is being tested by that section.Example of bad test,/*** tests for the addMonths() method.*/@Testpublic void should_add_Months() { var d1 = CustomDate.createInstance(31, 5, 2004); var d2 = CustomDate.addMonths(1, d1); assertEquals(30, d2.getDayOfMonth()); assertEquals(6, d2.getMonth()); assertEquals(2004, d2.getYYYY()); var d3 = CustomDate.addMonths(2, d1); assertEquals(31, d3.getDayOfMonth()); assertEquals(7, d3.getMonth()); assertEquals(2004, d3.getYYYY()); var d4 = CustomDate.addMonths(1, CustomDate.addMonths(1, d1)); assertEquals(30, d4.getDayOfMonth()); assertEquals(7, d4.getMonth()); assertEquals(2004, d4.getYYYY());}Rule 2: One Assert per TestThis rule says that every test function in a JUnit test should have one and only one assert statement. This rule may seem harsh, but the advantage can be seen in the below code snippet. Those tests come to a single conclusion that is quick and easy to understand.Let’s see one code example to understand this rule.Before:@Testpublic void should_return_page_hierarchy_as_json() throws Exception { makePages(\"PageOne\", \"PageOne.ChildOne\", \"PageTwo\"); submitRequest(\"root\", \"type:pages\"); assertResponseIsJson(); assertResponseContains( \"{ {\\\"page\\\": \\\"PageOne\\\"}, {\\\"page\\\": \\\"PageTwo\\\"}, {\\\"page\\\": \\\"ChildOne\\\"} }\" );}After:@Testpublic void should_return_page_hierarchy_as_json() throws Exception { givenPages(\"PageOne\", \"PageOne.ChildOne\", \"PageTwo\"); whenRequestIsIssued(\"root\", \"type:pages\"); thenResponseShouldBeJson();}@Testpublic void should_return_page_hierarchy_with_right_tags() throws Exception { givenPages(\"PageOne\", \"PageOne.ChildOne\", \"PageTwo\"); whenRequestIsIssued(\"root\", \"type:pages\"); thenResponseShouldContain( \"{ {\\\"page\\\": \\\"PageOne\\\"}, {\\\"page\\\": \\\"PageTwo\\\"}, {\\\"page\\\": \\\"ChildOne\\\"} }\" );}Do we have any real examples?Now, let’s see another example that is taken from the production code. Below is the code snippet of the unit test.@Testvoid test_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); assertEquals (Status.SUCCESS, wrapperResponse.getStatus()); assertEquals (\"ID0001\", wrapperResponse.getLosLeadId()); assertEquals (Status.SUCCESS.getCode(), wrapperResponse.getCode()); assertNull (wrapperResponse.getFailureType()); var clientCreationResponse = response.getClientCreation(); assertEquals (Status.SKIPPED, clientCreationResponse.getStatus()); assertEquals (\"10001\", clientCreationResponse.getCustomerId()); assertEquals (\"LOS0001\", clientCreationResponse.getUniqueRecordId()); assertEquals (Status.SKIPPED.getCode(),clientCreationResponse.getCode()); assertNull (clientCreationResponse.getFailureType()); var loanAccountCreationResponse =response.getLoanAccountCreation(); assertEquals (Status.SKIPPED, loanAccountCreationResponse.getStatus()); assertEquals (\"10001\", loanAccountreationResponse.getLoanAccountId()); assertEquals (\"1234\", loanAccountCreationResponse.getUniqueRecordId()); assertEquals (Status.SKIPPED.getCode(), loanAccountCreationResponse.getCode()); assertNul1 (loanAccountCreationResponse.getFailureType()); var loanContractCreationResponse = response.getLoanContractCreation(); assertEquals (Status.SUCCESS, loanContractCreationResponse.getStatus()); assertEquals (\"10001\", loanContractCreationResponse.getLoanContractId()); assertEquals (\"XXX123GYU\", loanContractCreationResponse.getUniqueRecordId()); assertEquals (Status.SUCCESS.getCode(),loanContractCreationResponse.getCode()); assertNull (loanContractCreationResponse.getFailureType());}As you can see, the above test code is very complex and has too many assertions. This test is hard to understand. As a matter of fact, the code that this test is written for is also very complex. As we know test is a documentation for the project/service, so the above test will confuse more developers instead of helping when things go bad. Let’s apply the earlier 2 rules discussed in the above test code....@Testvoid test_wrapper_status_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert assertEquals (Status.SUCCESS, wrapperResponse.getStatus()); assertEquals (Status.SUCCESS.getCode(), wrapperResponse.getCode()); assertEquals (\"ID0001\", wrapperResponse.getLosLeadId()); assertNull (wrapperResponse.getFailureType()); } @Test void test_client_creation_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var clientCreationResponse = response.getClientCreation() ; assertEquals (Status.SKIPPED, clientCreationResponse.getStatus()); assertEquals (Status.SKIPPED.getCode(),clientCreationResponse.getCode()); assertEquals (\"10001\", clientCreationResponse.getCustomerId()); assertEquals (\"LOS0001\", clientCreationResponse.getUniqueRecordId()); assertNull (clientCreationResponse.getFailureType());}@Testvoid test_loan_account_creation_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var loanAccountCreationResponse =response.getLoanAccountCreation(); assertEquals (Status.SKIPPED, loanAccountCreationResponse.getStatus()); assertEquals (Status.SKIPPED.getCode(), loanAccountCreationResponse.getCode()); assertEquals (\"10001\", loanAccountreationResponse.getLoanAccountId()); assertEquals (\"1234\", loanAccountCreationResponse.getUniqueRecordId()); assertNul1 (loanAccountCreationResponse.getFailureType());}@Testvoid test_loan_contract_Creation_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var loanContractCreationResponse = response.getLoanContractCreation(); assertEquals (Status.SUCCESS, loanContractCreationResponse.getStatus()); assertEquals (Status.SUCCESS.getCode(),loanContractCreationResponse.getCode()); assertEquals (\"10001\", loanContractCreationResponse.getLoanContractId()); assertEquals (\"XXX123GYU\", loanContractCreationResponse.getUniqueRecordId()); assertNull (loanContractCreationResponse.getFailureType());}...The above-refactored code looks pretty nice compared to the earlier test, but still, there are many asserts. A test should have at max 5 assertions per function but as a best practice, it should have a single assertion per test. if assertions count &gt; 5, it means you are doing something different and it is complex.Following a single assertion per test rule, we can further decompose our test. Let’s take test_something_for_client_creation test in the last step and further break it down into smaller tests.@Testvoid test_client_creation_that_has_skipped_status_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var clientCreationResponse = response.getClientCreation() ; assertEquals (Status.SKIPPED, clientCreationResponse.getStatus());}@Testvoid test_client_creation_that_has_skipped_status_code_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var clientCreationResponse = response.getClientCreation() ; assertEquals (Status.SKIPPED.getCode(),clientCreationResponse.getCode());}@Testvoid test_client_creation_that_has_customerId_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var clientCreationResponse = response.getClientCreation() ; assertEquals (\"10001\", clientCreationResponse.getCustomerId());}@Testvoid test_client_creation_that_has_uniqueRecordId_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var clientCreationResponse = response.getClientCreation() ; assertEquals (\"LOS0001\", clientCreationResponse.getUniqueRecordId());}@Testvoid test_client_creation_that_has_success_failure_type_for_something() { // Given ... // Action var wrapperResponse = loanCreationWrapperService.createLoan(wrapperRequest); // Assert var response = wrapperResponse.getWrapperResponseBody(); var clientCreationResponse = response.getClientCreation() ; assertNull (clientCreationResponse.getFailureType());}... // adding similarly for other testRule 3: Tests should not depend on each otherAs a general rule, each test function should contain all the code and resources that it requires to test the piece of code. A test function is a mini-universe that contains all things it needs to test the piece of code. For a particular class under test, there will be many mini-universe. A failure in one test shouldn’t affect the other test directly or indirectly.Example: In the below code snippet, we are using a static resource called Security context. It is used to handle user authentication and extract user details from the token. If we see carefully, we are creating a static mock of the SecurityContext.class and using the object to mock the method response.@Testvoid test_something() { ... var securityContext = Mockito.mockStatic(SecurityContext.class);\tsecurityContext.when(SecurityContext::getUserId).thenReturn(CUSTOMER_ID); ... securityContext.close();}If the above test fails and fails to release the securityContext static mock object, It will cause failure in other tests. And debugging will be tough if we are not aware of this problem. If you are not following proper development practices, there is a good chance that your code will be having similar issues. One way to solve the above problem is to use try-with-resources statement to handle this failure. Refer to the code snippet for the above code refactored to use try-with-resources statement.@Testvoid test_something() {\ttry (var securityContext = Mockito.mockStatic(SecurityContext.class)) {\tsecurityContext.when(SecurityContext::getUserId).thenReturn(CUSTOMER_ID);\t...\t}}The common reason for occurring this issue: Not following correct testing practices. Not releasing the static mock resource after completion of the test. Not handling commonly shared resources in testing.ConclusionBased on my observation, many developers generally don’t follow proper development practices. They either write production code before test code or don’t give importance to testing and put less effort into writing such tests. I have witnessed such a project, where the developer after a few years of development of the feature, hesitates to add new modifications with the excuse that code is complex, and he doesn’t have the context to add changes to that feature. It is a disaster for the project.It would be a lot better if proper development practices were followed in the project to minimize such an impact. Practices such as Pair Programming, Mob Review, Test Driven Development, Code Review, and Code Documentation are some practices to ensure high-quality code and less time to deploy new changes in production. You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Do not trust others' code", "url": "/posts/do-not-trust-others-code/", "categories": "Blogging, Article", "tags": "design, backenddevelopment", "date": "2022-10-31 20:00:00 +0530", "snippet": "In this article, I shared one instance of poor API implementation with numerous bugs and my experience with it.Background ContextAs per the current business process, If someone wants a loan against security, they have to physically visit one of the branches and wait in a long queue to start the process for the loan.The person who handles that request manually checks documents and later creates customers in the system once a loan is approved. Now, business wants to go digital and automate this manual process. Once the loan is approved digitally, the customer gets created in the same backend system (Legacy). This legacy system is core for most of the process and is difficult to change in a few years. So now you understand what problem we are going to discuss here.Client creation API is used to add a new customer once the loan against security is approved. This API already exists. In the orchestrated loan application journey, I have to consume that API once the loan is approved.The ProblemI am not able to create new clients in the legacy system. I read the documentation and reviewed the sample request numerous times but failed to create a new client via API.Let’s begin,Part 1: Status code misuse and misleading error messageMy initial feeling by looking at the API documentation was scary as it has more than 130 fields for the request in an excel sheet. Whenever I send a request for client creation, I get the below error,Status code: 500 INTERNAL SERVER ERROR{ \"Message\": \"An error has occurred.\"}After looking at the above response, I thought it did authentication successfully, and when it hits the service, something has gone wrong with the request, and because of that it is giving the above error.But, API requests have more than 130 fields in sample requests, and It is unclear from the response what is the actual problem. Even after trying different approaches , I failed to create a client, so I contacted a person from the integration team who successfully added a new client to the system, but that person used UI to create the client. Gauche, this is a pain. It’s like hitting the wall.One and half days have been wasted mailing, waiting, calling, reading documentation, and trying different approaches. I still have no clue why I am getting this error.Based on a hunch, I thought to check the request header and found the issue. In the request header, username and password field were added in the wrong way. After resolving, I was able to get a different error but yeah some progress. Instead of 401, they used 500 (INTERNAL SERVER ERROR ) for authentication failure. Is it the ignorance or laziness of the developer? This issue took me nearly two days. And, it is not the end.Part 2: API is badly implemented and poorly documentedNow, I can see different errors and resolve most of the issues by referring to the documentation and the database(I got access via UI to check).After resolving most of the validation errors, I got the below error.{ … \"status\": { \"Status\": \"Fail\", … \"Remarks\": \"Exception: Column ‘customerid’ does not belong to table Table.\" }}I may be sending customerId as an extra field, so I checked the payload again, but to my surprise, there is no field like customerId. As per the documentation, I do not see any problem with the request, so what is wrong here? 😫Hold on, why is the error saying the customer id column in the table is not present? Something strange is about the implementation of the API. It is revealing something which should not be. I wonder what database query is used to insert new client details.Since I can view the database, based on some trial and error with different combinations of values, I realized that the branch code (varchar) field with integer value works, but for string, it fails with the above error. But why is it even accepting that value if it should be an integer? why is it not part of the validation?It is not the only problem with this field. When I used values from the branch code column(Integer) already in the database, as per my understanding, it should work, but it is failing with the above error again. I do not understand this behavior of the API implementation.After some trial and error, I used the value from branchID instead of the branch code column in the database. It worked, but to my surprise, the documentation says branch code, but it is working with branch ID. I lost my trust in the API implementation.Part 3: I somehow managed to make the API work, but the default value, as per the documentation, is breaking the API call.It cost me approx. 2.5 days for making the first successful API call. It gave me mixed feelings of enjoyment and sadness. Next, I thought of playing with this API with some different values, which I will send to this API once I start the story implementation. I had some default values to use for my use case and thought to send that to the API and check whether I can successfully create a new client or not.The response I got looks like below,{ … \"status\": { \"Status\": \"Fail\", … \"Remarks\": \"Exception: Error converting data type varchar to numeric.\" }}From the above error, I thought maybe I edited the wrong field, but no, all field values look fine. So what could have gone wrong? The sad part about this error message is I do not know which among the multiple edited fields (10 fields I edited) has the issue. I had to undo the changes and try to send a new call after each field change. What a great API design and developer experience!, I was laughing with my pair.Later I found the problem I was giving groupName as NA (before groupName was Diamond), ok it is the name, so I should be receiving an invalid name or something like that, but why is the API internally trying to convert that to the numeric value for NA?Seriously, I have no idea what to say. Internally I was crying and thinking about running away.groupName field in the request now reverted to the previous value. Let’s see if I can send the request. Yes, I can create a new client in the system again. This time I felt true happiness in my last four days.Part 4: frugally removed unnecessary/optional fields from the request payload but now a weird validation error.Since API requests have numerous optional fields, I tried to remove which are unnecessary, as per my use case. I removed most of the fields except for one field(pool account). If I remove it and send the request, here is the response,{ … \"status\": { \"Status\": \"Fail\", … \"Remarks\": \" TaxStatus is required\" }}Maybe I accidentally deleted the tax status field, so I checked the payload, but it is present. Why am I getting the above error? So I tried to add back the pool account and sent the same request again, and it worked. What is happening here? Is this another problem in the API? Big Yes.The documentation says It is an optional field and can be blank. So I tried to remove it like other optional fields. I understand that sometimes we developers miss updating the documentation, which is ok, but why is it giving me the wrong error message? It should have said pool account required and not misleading by saying tax status.Part 5: Finally, I can send the request as per my use case, but my happiness did not last for more than 10 mins.I got the feedback to update the data via UI after creating a new client in the system as branch folks will be using that same system to update the data. It is fair, and I thought it would be a piece of cake as I only have to update one field in the UI and try to save the data as branch folks do.As I clicked the save button, I got numerous validation error messages. Yikes, what is going on here?…I was hoping the API would save the data in the system with all the proper validation. Is it too much to expect?Jokes apart, I manually had to resolve all those validation errors in the UI by adding the missing values and later adding the same in the API.During fixing those validation issues, I encountered one mandatory field called department, which is required but was missing. So I checked API documentation for where to pass in the request, but I did not find anything about it. So I checked sample requests but no luck. Only one question I had at that time, Why did the system even allow the saving of the new client data? If validation was missing, shouldn’t the API be responsible for validation? Well, it is easy to criticize others’ code. But maybe there could be some unknown reason for which this happened. What could you have done differently if you were in that developer’s place?What could be improved? Use proper documentation tool: The documentation which I got is nothing but excel sheets and some sample request payload in the text file. Seriously, we have a lot of good tools like swagger UI, Postman, Redoc, etc.; but still, why are we ignoring them and using word files, excel files, confluence, etc.? I think this decision is made by the tech team, not by business, so I feel this is the ignorance of one who is leading the team. Use proper testing: Everyone knows about unit testing, integration testing, and end-to-end testing, but sometimes we do not write a valid test. We get the feeling of 80%+ test coverage but does that even matter if your code cannot do one thing properly? In my case, if testing was fairly done, I might not have spent three days sending one successful request to your API. Validate content: If you are making any code changes, ensure to validate the documentation content. In my case, it was an excel sheet with 130+ fields for request. Give meaningful error response: It is better to focus on the error handling part of the API. Sadly, This is something I have observed developers give less importance to. If validation fails, return a proper reason for failure instead of one generic error. I do not have access to your code, and you should not expect me to find time to read your code before accessing your API. Do perform other business operations on data: In my case, client creation API by name suggests I should create and save in a database. But as a developer, I also need to ensure that other business operations on the stored data are working. For here, after saving, when I tried to update the data via UI, which branch folk will do, I got many validation errors that got missed in the API implementation. When bypassing the manual process, try to validate other business flows on data. ConclusionClient creation API is core in journey automation. From five short stories about the same API, you could have realized the pain of using a poorly implemented API. I have experienced a lot of pain with poorly implemented APIs and poor documentation, but this experience is different from others and unique.Want to read more? Presentation: How not to document your API? Blog: What makes API documentation terrible?" }, { "title": "How (Not) To Document Your APIs", "url": "/posts/how-not-to-document-your-apis/", "categories": "Presentation, Documentation", "tags": "design, backenddevelopment, softwareengineering", "date": "2022-10-23 00:00:00 +0530", "snippet": "How (Not) To document Your APIsIn this presentation, I talked about the troubles I faced due to poor API design. It happened due to less emphasis on documenting changes. Developers who do not have access to the internal code of the API will only have to rely on provided document.I shared four instances where poor documentation caused more pain than helping me to make a sound judgment about the effort required to integrate that API into my change.Image source: https://giphy.comVideo Deck " }, { "title": "State in reactive system - Orchestration-based Saga Pattern", "url": "/posts/saga-pattern-state-in-reactive-system/", "categories": "Presentation, system-design", "tags": "design, backenddevelopment, softwareengineering", "date": "2022-08-31 00:00:00 +0530", "snippet": "State in Reactive SystemIn this presentation, I talked about something we are already familiar. For some, it will be a refreshing session; for others, it will be new learning.We see how our design evolves as we progress with the presentation. We initially start with a simple application, later identify problems and see how to fix those problems.Video Deck Further readingsBlogs Modern day service orchestration using DSLs Service per team Microservices The Reactive Manifesto Saga PatternVideo: Spring I/O 2022: Be proactive about state in a reactive system by Nele Lea UhlemannBook: Practical Process Automation: Orchestration and Integration in Microservices and Cloud Native Architectures by Bernd Ruecker" }, { "title": "What makes API documentation terrible?", "url": "/posts/what-makes-api-documentation-terrible/", "categories": "Blogging, Article", "tags": "design, backenddevelopment, softwareengineering", "date": "2022-08-13 00:00:00 +0530", "snippet": "Is this another article on How to write documentation?No, this article is not about How to write API documentation but about what things everyone should avoid while writing API documentation. This documentation is meant to be used by other developers, so it is better to avoid the below problems that I tried to articulate.What do we mean by API in simple terms? An interface through which clients interact with the services.Why am I asking about API? There is a reason I brought up this topic. I have read a lot of API documentation internal and external(vendor API) to the projects, and the majority of them are poorly maintained. Sometimes, it is hard to understand what data to send for the working of the API. Why don’t developers give importance to writing API documentation? Why is it still hard to understand the API even after reading the documentation provided by the team who developed that API? Let me share the problems I faced while working with other APIs. This problem is something I have experienced in many projects internally and with vendor-provided solutions.Instance 1: Account Details APIThis API is used to get account details based on customer ID (Required) and account id (Optional). API says the customer ID field is mandatory, but if not provided still works. 😑 If only an account ID was provided, which is optional, it can still return details based on the optional field. 😑 Secondly, it also returns other resources which have matching account IDs but different customer IDs. It was not documented for the API. Even if it returns details for multiple resources, there is no way to identify which resource belongs to which customer ID as there is no field ID in returned resource details. Seriously, Which to use? How am I going to identify it? It makes no sense to me. 😔 Instance 2: Suspension Details APILegacy API is used to get the suspension details for customer id. API is named GET BASE_URL/suspension-details, but it returns account details. Due to bad naming, it created more confusion during migration. It should have been named GET \"BASE_URL/account-details. Why did this thing get missed? 😕 Instance 3: Money Transfer APIAn API used to send money from an internal account to an external account (customer account) API has some fields marked as mandatory in the documentation, but it can accepts empty/null values. It doesn’t make sense. What is the point of mandatory fields? Details for debtor and creditor accounts have all the fields marked as optional. In reality, as per the business, that API needs mandatory details, but technical implementation says different things. I had to make a few calls with the client team and check the usage in other products to understand the API. In the name of adding encryption features in the API, many fields are renamed, new fields were added, and some were removed. I don’t understand why this is not communicated to the other team/business, as this leads to the wrong information communicated with developers. Later, wrong estimations are given by developers.This is a serious issue. Instance 4: Product Rate APIAn API used to update the product rate. This rates get updated multiple times in a day. The data model name used for serializing the request payload looks like the TOCard, but what does TO mean here? No documentation was available, and When I checked the Jira story, there were also TO terms used. So, what does “TO” mean? I was clueless. In business terms, there was not any TO-related jargon used. The team that built that API is no longer available. During migration, new developers picked it up, but they had no full context. Going directly to the product owner could be a good approach, but I wanted to know why that developer thought TOCard name is sufficient to understand the payload data. Once data is updated, it returns the response which has a message like, Successfully updated TOCard details Later I realized after talking with a few old developers that “TO” in TOCard may mean Treasury Officer, as there is one such department. But, it is again a guess by that developer. I do not understand what that developer achieved by saving a few characters in the model name. It is a useless name as it is not directly helping me understand the API. Instance 5: Event ProducerInterface does not have to be REST API. It could be a Pub-Sub type interface, where the requests in the form of a message are consumed, and a response is sent in the form of a message/event. Here, Incomplete implementation of producer pushed in production that wasn’t consumed by any product. So, it did not affect any product. What is the meaning of that code? Why was that code not reverted? It gave me the understanding that since it is in prod, It must be working. After I integrated my changes to consume that message, my code won't get it. After debugging the framework, I found that the header in the message had one mandatory field missing. After fixing it worked. Seriously, for someone new to the framework, this is a nightmare as he could not know framework level understanding in a short time.Better to implement it correctly or avoid writing it in the first place. Instance 6: 2 Factor Authentication (2FA) APIAn API is used to send OTP messages to the customers, and another API is used to validate OTP messages. Validating OTP requires a sent OTP pin and function ID(used to identify message format specific to the product).The ProblemOTP Validation API suddenly stopped working for the product I was working on, which caused smoke failure in the lower environment. It blocked us from pushing any change to the higher environment like QA.BeforeThe default value used for function ID in OTP generation and validation. This change was working fine. The front-end pulls the function ID from Database(DB) based on product code.NowAfter eight months, the frontend code had two different values for the product code (for example, FOREX and FOREX_CARD). The frontend code was quite complex to check which config was used while making the OTP generation and validation call, as two different configs were present with different product codes. Later checking the DB, I found that the product code got updated from FOREX to FOREX_CARD a few days ago. In the OTP validation API, I found that if the function ID is missing in the request body, it takes the default value. Next, talking with the integration team, I learned that the function ID has to be the same in OTP generation and validation calls for a product code which was a different value in the API call. Later after 3-4 hr debugging, I realized that the frontend was using an older name (FOREX), not the new name(FOREX_CARD) to get the function ID. By looking at the frontend code, it was difficult for me to understand those things, but with the help of frontend folks, I found the config. Questions in my mind, Why were there two different names for product code in the frontend code? Why is older code not removed? Anyway, front-end code was refactored and removed code related to older names. Afterward, It started working. It is the code and unclear understanding of the API usage due to poor documentation that caused the issue. The function ID has to be the same in OTP generation, and validation calls for a product code This was not documented in tools like Jira, confluence, etc. Later talking with the integration team, I came to know.Conclusion I will not blame the team, but the practice that was followed by the developer who implemented that API in the product should have mentioned necessary details in the story so that others could quickly come to know. It is better to have a clear understanding of API before making any changes to the product. Small changes could go unnoticed, and no one will come to know the actual cause of failure unless the one who implemented that change is debugging the issue, which in my case was not available. Documentation is there to help other developers transition to your API smoothly. It is also better to review your documentation with other peers if you are not clear about what to include and add. This documentation saves a lot of hours and unnecessary meetings and calls for small things. It is better to add implicit details in the documentation not directly mentioned in the code but is required to use the API correctly. Avoid using meaningless small names or names like TOCard, which won’t help anyone understand your API. If those terms are business defined better to add a description explaining in 1-2 lines. Double check the details regarding what fields are optional and mandatory, as miss information could cause the story to spill over.   You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Code Smell Series: Shh…something is happening there", "url": "/posts/code-smell-series-ssh-something-is-happening/", "categories": "Blogging, CodeSmellSeries", "tags": "coding, smells", "date": "2022-07-02 00:00:00 +0530", "snippet": " …Shh…let me tell you…something is happening in that code.Last month, I encountered one problem in the code. That code act as a decision-maker to decide the next step in the application, based on the decision from the human(Team Dispatcher), which is an async task and takes time.The ProblemThe problem was that decision-maker implementation was complex and was doing many things. Things were happening behind every line of code, Which is difficult to understand even if you read that code multiple times.Let’s jump to the code,class DispatcherDecisionMaker( private val onApprove: Step, private val onReject: Step, private val onPendingSettlement: Step) { fun onExecute( dispatcherHumanTaskResponse: DispatcherHumanTaskResponse? = null, fetchPaymentResponse: FetchPaymentResponse? = null ): Step { return when (fetchPaymentResponse == null) { true -&gt; getDispatcherResponse(dispatcherHumanTaskResponse) false -&gt; when (fetchPaymentResponse.isPaymentSettled) { false -&gt; onPendingSettlement else -&gt; getDispatcherResponse(dispatcherHumanTaskResponse) } } } private fun getDispatcherResponse(dispatcherHumanTaskResponse: DispatcherHumanTaskResponse?) = when (dispatcherHumanTaskResponse == null) { true -&gt; onApprove false -&gt; when (dispatcherHumanTaskResponse.decision) { CaseDecision.APPROVED -&gt; onApprove else -&gt; onReject } }} Even today, After reading the above code and knowing the flow, I still need to check steps and other implementations to understand the code. I encourage you to once read the above code and then think about what you understood?UnderstandingIf you are unable to understand the above code, you are not alone. Let me brief you on what checks the above code does? Is payment settlement toggle/flow enabled? Was there any response from the fetch payment status API call? If there was any response, check for payment settlement status and decide whether to wait for settlement to complete or move on to dispatcher(Team) decision? Is dispatcher human task toggle/flow enabled? If it is not enabled, default to as approved. Otherwise, check the decision made by the Team Dispatcher and decide on the next step.I was surprised that too many things were happening and have been taken care of by the above piece of code. But reading the code again now, I still feel confused as it is implicit knowledge, and the other developer might not have that proper context, so he/she again needs to read the last step from the code itself.From the code, we see the class is handling two responsibilities Payment decision and Dispatcher decision. Also, there is naming smell problem in getDispatcherResponse(). getDispatcherResponse() method doesn’t return the dispatcher response but returns the next step to execute based on the human decision from Dispatcher Team.fetchPaymentResponse == null, this code check whether payment settlement flow/toggle is enabled or not. Seriously, I can’t interpret this by reading this class alone. That developer will have a hard time understanding his code after a few years.What is the use of else in line number 15?, Why not true? it makes more sense than writing else there.- else -&gt; getDispatcherResponse(dispatcherHumanTaskResponse)+ true -&gt; getDispatcherResponse(dispatcherHumanTaskResponse)dispatcherHumanTaskResponse == null, this code checks whether Dispatcher Human Task flow/toggle is enabled or not. Again, this is hard to understand from the code. I am scared of the day if that change got pushed into production, and after a few months, some strange bug appears. I can’t imagine that scenario for the developer who will be debugging it as everything is happening as an event.RefactoringNow, Let’s see refactored version of the above implementation.class DispatcherDecisionMaker( private val onApprove: Step, private val onReject: Step, private val onPendingSettlement: Step) { fun onExecute( dispatcherHumanTaskResponse: DispatcherHumanTaskResponse? = null, fetchPaymentResponse: FetchPaymentResponse? = null ): Step { return when (isPaymentSettlementEnabled(fetchPaymentResponse)) { false -&gt; nextStep(dispatcherHumanTaskResponse) true -&gt; when (isPaymentSettled(fetchPaymentResponse)) { false -&gt; onPendingSettlement true -&gt; nextStep(dispatcherHumanTaskResponse) } } } private fun isPaymentSettlementEnabled(fetchPaymentResponse: FetchPaymentResponse?): Boolean { return fetchPaymentResponse != null } private fun isPaymentSettled(fetchPaymentResponse: FetchPaymentResponse?): Boolean { return fetchPaymentResponse!!.isPaymentSettled } private fun nextStep(dispatcherHumanTaskResponse: DispatcherHumanTaskResponse?) = when (isDispatcherFlowDisabled(dispatcherHumanTaskResponse)) { true -&gt; onApprove false -&gt; when (dispatcherDecision(dispatcherHumanTaskResponse)) { CaseDecision.APPROVED -&gt; onApprove else -&gt; onReject } } private fun isDispatcherFlowDisabled(dispatcherHumanTaskResponse: DispatcherHumanTaskResponse?): Boolean { return dispatcherHumanTaskResponse == null } private fun dispatcherDecision(dispatcherHumanTaskResponse: DispatcherHumanTaskResponse?): CaseDecision { return dispatcherHumanTaskResponse!!.decision }}Further RefactoringAbove code is still doing multiple things. It can be further simplified into 2 separate class,Implementation of Dispatcher Decision Maker:class DispatcherDecisionMaker ( private val onApprove: Step, private val onReject: Step) { fun onExecute(dispatcherHumanTaskResponse: DispatcherHumanTaskResponse? = null): Step { return when (isDispatcherFlowDisabled (dispatcherHumanTaskResponse)) { true -&gt; onApprove false -&gt; when (dispatcherDecision (dispatcherHumanTaskResponse)) { CaseDecision.APPROVED -&gt; onApprove else -&gt; onReject } } } private fun isDispatcherFlowDisabled (dispatcherHumanTaskResponse: DispatcherHumanTaskResponse?): Boolean { return dispatcherHumanTaskResponse == null } private fun dispatcherDecision (dispatcherHumanTaskResponse: DispatcherHumanTaskResponse?): CaseDecision { return dispatcherHumanTaskResponse!!.decision }}Implementation of Payment Status Decision Maker:class PaymentStatusDecisionMaker( private val onPendingSettlementFlow: Step, private val onSuccess: Step, private val onSkip: Step){ fun onExecute( fetchPaymentResponse: FetchPaymentResponse? = null ): Step { return when (isPaymentSettlementEnabled (fetchPaymentResponse)) { true -&gt; when (isPaymentSettled(fetchPaymentResponse)) { true -&gt; onSuccess false -&gt; onPendingSettlementFlow } false -&gt; onSkip } } private fun isPaymentSettlementEnabled (fetchPaymentResponse: FetchPaymentResponse?): Boolean { return fetchPaymentResponse != null } private fun isPaymentSettled(fetchPaymentResponse: FetchPaymentResponse?): Boolean { return fetchPaymentResponse!!.isPaymentSettled }}It looks much better as each class is doing one thing. Even though it required to create another class but at the cost of clean responsibility and better readability, Which has more advantages than disadvantages. Also, it increases reusability.Conclusion:Initially, we saw that code was doing more things that can’t be easily interpreted by just reading it. To get the full context of the whole code, I needed to read the previous flow, and then I had to figure out what was happening, which was a clear signal of code smell. Be careful next time.  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Code Smell Series: Naming smells - Part 2", "url": "/posts/code-smell-series-file-name/", "categories": "Blogging, CodeSmellSeries", "tags": "coding, smells", "date": "2022-06-19 00:00:00 +0530", "snippet": " …Ahem…Why I still see non-sense naming?I recall encountering a few naming smells in the legacy system, during migration activity. There I saw three files with similar name in the production infrastructure code.ContextAs per new business requirement, I had to make some enhancements to the existing service to use different authorization tokens based on environments. Config/Code level changes were pretty straight forward.I encountered an actual problem when I wanted to deploy my changes to a non-dev environment (like release assurance and Prod). There, I needed to add some additional changes to Jenkins credentials, deployment pipeline code, and helm chart.The ProblemProd.yamlProd-new.yamlProd-new-new.yamlI saw the above three files for both release assurance and production environment. In that legacy system, no active development was happening. When I checked git logs for the latest changes, I failed to figure out which file I needed to change. Only one of those configurations is in use in the deployment pipeline.Which one is it?After checking the commit, I saw all three was updated same time. It is weird. Why does someone updated all three files when only one is in used?You may say, It could be to have a backup of the configuration. As git is been used for a long time, I don’t see any reason for the backup. If you want to revert to 1 month older configurations, you can easily do it.Ok, I already spent more than 20 mins, and I was thinking prod.yaml should be the one, but to double confirm, I checked with the infra folks for what file I have to update, and there I came to know I have to update prod-new-new.yaml.What? Why this weird naming format? I don’t understand this naming convention. This naming is adding confusion instead of bringing clarity.I asked the same question, but they were saying, Pravin that been there for a long time, and what can we do? I don’t understand How come that change at that time got approved.  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Code Smell Series: Naming smells - Part 1", "url": "/posts/code-smell-series-variable-name/", "categories": "Blogging, CodeSmellSeries", "tags": "coding, smells", "date": "2022-06-15 00:00:00 +0530", "snippet": " Are you sure your code is free from naming smell?You may think about ignoring it and adding to backlog due to week smell, but if that tech debt is not fixed and delayed, it will create more headache then peace of mind while working. It is the responsibility of developers to make sure that the code they are writing should not include naming Jargon, whether it is the application name, API endpoint, module, class, method, variables, etc. Until now, I have encountered this problem many times, but majority of them were small, had less impact and were fixed within a week. I recall 2 different instances which troubled me more.RecentlyI encountered naming smell, while performing application migration from one platform (in-house old service platform) to another platform (in-house new advanced service platform). There was one API that was exposed to the back-office team for pushing the latest data multiple times a day. The API accepts a list of currency exchange details that is further processed (aggregation, validation, transformation, and more) and then stored in the DB to be used by other applications.Problem 1:Here, naming of a few fields in currency exchange was not clear, something like, fxdCrncyCode and varCrncyCode.Someone who is reading this name for the first time will not come to know what it could mean. Is it some business term? We could guess Crncy could roughly mean currency given we are accepting currency exchange detail. But, what could fxd and var mean here?I tried asking few developers who joined project before me, but they don’t know as that service was last time updated a year ago, and folks who developed is no longer working in the project. So next, I thought to check the API contract testing code where I was expecting to find the test for that field and get actual meaning behind that, but sadly, I didn’t get anything there.At last, I had to read the complete API implementation to understand what was happening?, Where data was saved?, and there I saw it. Fxd mean fixed and var mean Variable.Now you will say, Pravin You could have guessed, it does roughly means same variable name.But hold on, Why do I have to guess the name? Why not written clearly in the first place? What did they achieved by saving two char from fixed and five char from Variable? One thing is for sure I had to spend two days just searching, asking, reading the code, and also confirm the same in the downstream application. The sad part is, the developer who implemented is no longer in the project, and no developer on the team had the full context about it. You could have checked the API documentation and come to know everything about it.That’s true. But, I was not able to find the documentation page in confluence. You could have checked details from the source that pushes the data to the API. Yes, I could have, but that might have cost me another 2-3 days as the team who uses the API is not actively aware much about the fields.API integrated with them three years ago, so not everyone in that team had full context. Also, the team was busy with many tasks at that time. Given the tight deadline, that could be the last option for me.Now What?If that developer had written the proper name, which was a simple task, It might have saved my precious time.It is easy to criticise others code. What would you have done different in that developers place? Lets discuss the possible solution to above problem. Some more context: After some time, I realised that developer used the same name coming from the source system in Request Payload.You may say, from the source, we are getting data in that format, so what could be done? Are you sure there is no option other than using fxdCrncyCode and varCrncyCode that is unclear?I think that is just an excuse. Below are the few approaches that, You could follow to avoid above problem.Approach 1:Create DTO object which accept the source data and translate to domain specific name like,data class ExchangeRateDTO ( val fxdCrncyCode: String, val varCrncyCode: String ) { fun toDomain(): ExchangeRate { return ExchangeRate( fixedCurrencyCode = this.fxdCrncyCode, variableCurrencyCode = this.varCrncyCode ) }} But this creates an extra class to manage.Approach 2:Much simpler way, if you are using Jackson like library for deserializing the data back to object,Use @JsonProperty in the data class field name to map the name to different name.data class ExchangeRate( @JsonProperty(\"fxdCrncyCode\") val fixedCurrencyCode: String, @JsonProperty(\"varCrncyCode\") val VariableCurrencyCode: String) In the API contract, Jackson will see that field name and set that against the proper name.Approach 3:Ask the source to send the correct field name. That could have been a good approach if done initially. But in my case, that contract was already in use, and the team sending the data was lazy to pick this tech debt.Problem 2:This API had the wrong endpoint. It was like ../card-rate, but in reality, it was not accepting card rate but currency exchange rate. So, Why it was not fixed later?Due to these everywhere in the code base, wrong names was used, and people who read the code thought, in a business context, it could mean card rate.But, the business people are not referring that name. so it is another naming smell in the API enpoint. Naming smell - Part 2, is on second instance which troubled me.Link to a page You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Tips to test (end to end) library changes - Gradle based Project", "url": "/posts/tips-to-test-library-changes/", "categories": "Blogging, Article", "tags": "gradle, softwareengineering, backenddevelopment, testing", "date": "2022-06-03 11:40:00 +0530", "snippet": "The ProblemI had made multiple changes in 2 dependencies of the spring application. These changes are on the local machine, and I need to test the functionality of the change end to end with the dependent application before pushing the artifact to a remote artifact repository like Artifactory.I know this is a simple task, but the reason is that some people are still struggling with this thing(including me), and now even popular IDE like IntelliJ also support that. But, which approach to choose is stillpeople get confused.&nbsp;SetupHere, I am using Kotlin, Gradle, and IntelliJ development IDE. For the sake of simplicity, I am going touse a simple project, but the challenges are still the same.Let’s startI ask about above problem every time to myself, whenever I am working on a project that has some changes in one of the dependencies, and I know there are many ways to handle it. Let’s see some of them.&nbsp;Approach 1: Upload the changes to a remote artifact repository (Artifactory) and use that to testThis is straightforward. Here, we simply publish our change to remote Artifactory and update the version of the dependency in build.gradle file,For example,-implementation(\"dev.pravin:projectb:1.0.1\")+implementation(\"dev.pravin:projectb:1.0.2\")I do not suggest this approach. If you are unsure about your change, it is better to avoid this approach. You could follow either of the below approaches to test locally. Later go ahead and follow this approach. Mostly this is taken care of by CI build pipeline.Other things to know: Once the build is available in remote Artifactory, we have to update the library version in the build.gradle file and use that to test the changes. It takes a lot of time as now we have to update the build version push those changes to the version control system (GitHub, Bitbucket, etc.), create a new build with CI pipeline, and later push it to Artifactory. Update the build.gradle file in the dependent app to test it. As we add changes, we have to follow the same steps again. It is easy to accomplish. The artifact repository becomes messy as there is no way to tell stable and unstable build. It creates more confusion for other folks who will be using it.&nbsp;Approach 2: Add changes as external jar dependencyIt is another way to add a dependency to the project, but it requires a few extra steps to configure the dependency. If you update the dependency, you need to remove the older one, then add the latest one.These steps keep repeating for each change.steps to add external jarOther things to know: We have to uncomment the current dependency for the build.gradle file and import the jar as an external dependency. We must create a jar of the library, and later follow steps to manually remove the old and add the updated jar. This approach can be frustrating if you have to test changes multiple times for the same library. It is easy to recall those steps, and anyone can easily do it.&nbsp;Approach 3: publish changes to the local maven repository(~/.m2) and using that to test changes Let’s say, Project A dependents on Project B.Changes in Project B (Assuming Project B already has required changes): Open the build.gradle file and add the following changes: Increment project version We can locally use it to identify artifacts by incrementing the version (like from 1.0.1 to 1.0.2). Add maven-publish plugin plugins { ... id \"maven-publish\" } .. Add below config for publishing ... publishing { publications { maven(MavenPublication) { from components.java } } } Screenshot of build.gradle in project B open the terminal in the root folder of the project and run gradle publishToMavenLocal. This command, creates a new build of the project with the updated version, and publishes the jar artifact in the local repository of the maven. This repository is used by other projects to resolve the dependency. Screenshot of terminal in project B Changes in Project A: we have to make small changes in build.gradle file. check for repositories and add “mavenLocal()” as the first entry. Next, we must update the dependency version to the one we published in the previous step. - implementation(\"dev.pravin:projectb:1.0.1\") + implementation(\"dev.pravin:projectb:1.0.2\") Screenshot of build.gradle in project A Reload the build.gradle changes and run the project. This time Gradle will check the maven local repository (~/.m2) and use that to resolve dependencies. We should now be able to run the application with the latest changes. Screenshot of project A execution output Here is the complete step animation: complete steps for both project changes It is much simpler and avoids unnecessary artifact upload to a remote artifact repository for each change.Other things to know: It has some code changes like adding the maven Local plugin, updating the version, and config related to publishing to maven local in build.gradle of the application. It comparatively takes less time, and it will be simple as we make changes.We can reuse the same version while publishing to maven local. In the end, add code changes, publish to maven local with the same version, and reload build.gradle in the dependent application. There are small code changes, and it is easy to follow.&nbsp;Approach 4: Add the library as a Module dependency in the application and test itThis pattern is followed by most Gradle projects where we add projects (Project A, Project B) as a subproject in the multi-project setup. We have to create top-level settings.gradle which has details of both subprojects. In our problem, this method doesn’t provide much benefit as it requires more changes in gradle files to test the code changes. It feels like using a bazooka to kill a fly. For step-by-step details, follow this linkOther things to know: We have to make a few changes in the Gradle file (build.gradle and settings.gradle) to achieve this. It takes time to set up for beginners, and even for non-beginner, it still requires following some documentation (In short, it requires effort to remember steps). Understanding how this works takes some time. It is done as a one-time setup. Due to more steps involved if wrongly followed, it may cause other problems like dependency not being visible, and there could be more. It is not simple, and checking changes with more applications could be time-consuming.&nbsp;ConclusionAfter going through all the above approaches, it is clear that Approaches 1 and 4 are not a good choice. Both approaches 2 and 3 are better for this problem statement.If there are small changes that we need to test, we can go with Approach 2, and it is ok as that involves fewer gradle file changes compared to approach 3.If there are many changes that you need to test, it would be better to go with Approach 3. Even though approach 3 has few extra changes compared to approach 2, we know that there is a good possibility that we again have to make changes and redo the same process. So, approach 3 reduces the hassle of manual importing that approach 2 does.For this problem statement, approach 3 is the better option as I need to make further changes and update the code. You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Understanding Nodejs(V8) and Spring Webflux(Netty) — based on Event Loop design", "url": "/posts/understanding-nodejs-v8-and-spring-webflux-netty-based-on-event-loop-design/", "categories": "Blogging, Article", "tags": "nodejs, springwebflux, backenddevelopment, eventloop", "date": "2022-04-14 00:00:00 +0530", "snippet": "For the last five months, I have been using Spring Webflux in my project for building asynchronous reactive Apps. In the past, I have also worked on the Nodejs project.In the below article, I tried to explain Nodejs and Webflux based on Event Loop, which gave me good insight into internal components and request handling. Full article was published on Medium, link to Medium blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Approaches for Start and Resume Journey for User Onboarding to Platform", "url": "/posts/approaches-for-start-and-resume-journey-for-user-onboarding-to-platform-part-i/", "categories": "Blogging, Article", "tags": "design, softwareengineering, backenddevelopment, onboardingprocess", "date": "2022-03-20 00:00:00 +0530", "snippet": "User onboarding to platform is very important process before they access services offered by the platform. The most important part is code sharing which becomes problem at some point if right design, data structure is not used. It is not an easy process, and in the below article (2 Parts), I have shared few approaches which can be used to handle this complex process. Part 1: Full article was published on Medium, link to Medium blog article Part 2: Full article was published on Medium, link to Medium blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Challenges in Migration of Application to Another Platform", "url": "/posts/challenges-discovered-in-migration-of-application-to-another-platform/", "categories": "Blogging, Article", "tags": "backenddevelopment, softwareengineering, migration", "date": "2022-02-12 00:00:00 +0530", "snippet": "I am working on the migration of an application from one platform to another platform, both were developed in-house, and managed by internal teams. The purpose of migration is to decommission the current platform, switch to another platform that has better features available to use, and have good integration with external services.Here is my another article on migration. In this article, I have discussed some of the common challenges.Let me know your thoughts 😉 Full article was published on Medium, link to Medium blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "AWS API Gateway — Ways to handle Request Timeout", "url": "/posts/aws-api-gateway-ways-to-handle-request-timeout/", "categories": "Blogging, Article", "tags": "aws, softwarearchitecture, backenddevelopment", "date": "2021-10-25 00:00:00 +0530", "snippet": "AWS cloud services are improving with time, and I am enjoying using their services. We all know that using 3rd part services in our application adds additional constraints to our services. Compared to benefits, those restrictions may sometimes be manageable.The Problem:I have come across a problem where API request timeout is restricted due to vendor (AWS in my case) and can’t increase after max timeout value (30 sec). Daily multiple batches ran by different API consumers that experienced timeout for some requests amounting to 3–4% of the total request sent.I have discussed above issue in depth in below article 😉. Full article was published on Medium, link to Medium blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Understanding Legacy System Migration using Strangler Fig Pattern", "url": "/posts/understanding-legacy-system-migration-using-strangler/", "categories": "Blogging, Article", "tags": "softwaredevelopment, systemdesign, strategy", "date": "2021-09-10 00:00:00 +0530", "snippet": "Strangler Fig Pattern appears to be straight forward strategy for migration of Legacy System to Modern Application. But in reality, it is not that easy process, and there are many variations of the same pattern. In this article, Strangler Fig Pattern has been discussed and steps to migrate legacy systems with a low risk of failure Full article was published on LinkedIn, link to LinkedIn blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Things I learned from analysis for migration activity", "url": "/posts/things-i-learned-from-analysis-migration-activity/", "categories": "Blogging, Article", "tags": "design, softwaredevelopment, nodejs, backenddevelopment", "date": "2021-08-16 00:00:00 +0530", "snippet": "I have been performing some analysis for nodejs application migration for the past few months. This activity has helped me identify problems and challenges that we usually face due to the poor design of some of the legacy app/libraries. For detailed insight, refer to the below article. Full article was published on LinkedIn, link to LinkedIn blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "External Traffic handling in AWS Kubernetes Services", "url": "/posts/external-traffic-handling-aws-kubernetes-services/", "categories": "Blogging, Article", "tags": "awscloud, kubernetes, networkarchitecture, scalability, resiliency", "date": "2021-06-27 00:00:00 +0530", "snippet": "I have been wondering, How AWS handles the traffic from external services to services/pods running inside the Kubernetes cluster? 🤨 In this article, we discuss what happens behind the scene in AWS Kubernetes and other options that can help for scalability and resiliency. Full article was published on LinkedIn, link to LinkedIn blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Understanding Class Loading in Java Virtual Machine", "url": "/posts/understanding-class-loading-java-virtual-machine/", "categories": "Blogging, Article", "tags": "java, security, backenddevelopment", "date": "2021-05-24 00:00:00 +0530", "snippet": "I had a use case where I wanted to update the implementation of the algorithm based on the trigger without restarting the app. This app is written using java(spring) and it exposes API to be consumed. Here, whenever the configuration is changed in the repository, the app is notified to reload the code. Java allows us to use our implementation of the class loader and using this, I implemented the required functionality.This article is created to explain the class loader and how we can use it to create our custom implementation and perform hot deployment without any app downtime. Full article was published on LinkedIn, link to LinkedIn blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" }, { "title": "Understanding Java VM for Memory Management", "url": "/posts/understanding-java-vm-memory-management/", "categories": "Blogging, Article", "tags": "java, docker-compose, kafka, performance-tuning, jvm, backend", "date": "2021-05-15 00:00:00 +0530", "snippet": "I had created a demo application in my free time which uses pub/sub for sharing messages between 2 java microservice application using Kafka as middleware. Later on, I starter experimenting on effect of different garbage collection algorithm using some popular tools.After trying out different algorithm, I am surprised that same code could get faster if right garbage collection algorithm is configured for the application.I have created an article to document same thing. Full article was published on LinkedIn, link to LinkedIn blog article  You have something to share, go ahead and add in comments 😉 » Happy Learning!!" } ]
